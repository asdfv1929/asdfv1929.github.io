<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[LeetCode 001 TwoSum]]></title>
    <url>%2F2018%2F06%2F19%2Fleetcode-001-twosum%2F</url>
    <content type="text"><![CDATA[LeetCode刷题，记录学习 问题给定一个整数数组和一个目标值，找出数组中和为目标值的两个数。 你可以假设每个输入只对应一种答案，且同样的元素不能被重复利用。 示例：1234给定 nums = [2, 7, 11, 15], target = 9因为 nums[0] + nums[1] = 2 + 7 = 9所以返回 [0, 1] 解答方法一：暴力法遍历数组中每个元素，查找是否存在一个值与target-x相等的目标元素123456789101112class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; for(int i = 0; i &lt; nums.length-1; i++) &#123; for(int j = i+1; j &lt; nums.length; j++) &#123; if (nums[j] == target - nums[i]) &#123; return new int[] &#123; i, j &#125;; &#125; &#125; &#125; throw new IllegalArgumentException(&quot;No two sum solution&quot;); &#125;&#125; 复杂度分析： 时间复杂度：$O(n^2)$$$\sum_{i=0}^{n-2}\sum_{j=i+1}^{n-1}1 = \sum_{i=0}^{n-2}(n-1) - (i+1) + 1 \\\\= \sum_{i=0}^{n-2}(n-i-1) = (n-1) + (n-2)+ … + 1 = \frac{n(n-1)}{2}$$ 空间复杂度： $O(1)$ 方法二：未完待续]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>Array</tag>
        <tag>Hash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter 02 How To Talk About Data in Machine Learning]]></title>
    <url>%2F2018%2F06%2F05%2FMasterMLA-Ch-02%2F</url>
    <content type="text"><![CDATA[Master Machine Learning Algorithms 一书 第2章节内容翻译。数据在机器学习中起着重要的作用。在讨论数据时，理解和使用正确的术语是非常重要的。在本章节中你会发现如何在机器学习中描述和讨论数据。阅读本章后，你会知道： 在讨论数据电子表格时，一般使用标准的数据术语 统计中使用的数据术语和机器学习的统计视图 在机器学习的计算机科学角度中使用的数据术语 这将大大帮助你理解一般的机器学习算法。 如你所知的数据你如何看待数据？想一下电子表格，其中有列、行和单元格。 列：列描述的是单个类型的数据。例如，你可以有一列重量、高度和价格。一列中的所有数据具有相同的比例，并且拥有相对于彼此的含义。行：行描述的是单个实体，列则是实体的属性。你拥有的行越多，则拥有的问题域中的样例越多。单元格：单元格是行和列的单个值。它可能是一个实数值（1.5）、一个整数值（2）或一个类别（red）。 这就是你可能会考虑的数据，列，行和单元格。通常，我们可以称这种类型的数据为表格数据（tabular data）。这种格式的数据很容易在机器学习中应用。机器学习有不同的风格，可以在这个领域上带来不同的视角。例如，有统计学角度和计算机科学角度。接下来我们将看到不同的指代数据的术语。 统计学习角度在统计学角度中，是在机器学习算法试图学习的假想函数（f）的环境下，构建数据。也就是说，给定一些输入变量（输入），预测的输出变量（输出）为：$$Output = f(Input)$$那些作为输入的列被称为输入变量。那些你可能并不总有的数据列，且在将来想要为新输入数据预测的数据称为输出变量，也称为响应变量。$$OutputVariable = f(InputVariables)$$ 通常，你有多个输入变量。在这种情况， 输入变量组可称为输入向量。$$OutputVariable = f(InputVector)$$如果你在之前有学过一些统计方面的内容，你就会知道另一个更传统的术语。例如，统计学中一些文本可能会将输入变量称为独立变量，输出变量称为因变量。这是因为在预测问题中，输出依赖于输入或自变量（独立变量）的函数。$$DependentVariable = f(IndependentVariables)$$ 数据在等式和机器学习算法中均使用简短的描述。在统计学中标准速记法试讲输入变量记为大写的x（$X$），输出变量为大写的y（$Y$）。$$Y = f(X)$$当你有多个输入变量时，可以用一个整数来表明它们在输入向量中的索引位置，例如，X1、X2、X3，分别对应前三列。 计算机科学角度从统计学的角度来看，数据在计算机科学术语中有很多重叠部分。我们可以来看下几个关键的区别。行通常描述的是一个实体（例如，一个人）或是对一个实体的观察。行中的列通常被称为观察的属性。当对一个问题进行建模和做出预测时，我们可能会参考输入属性和输出属性。$$OutputAttribute = Program(InputAttributes)$$ 关于列的另一个名称则为特征，用于与属性相同的原因，其中一个特征描述的是观察的一些属性。在使用必须从原始数据中提取特征构建观察的数据时，这种情况更为常见。这方面的样例，包括模拟数据，例如，图像、音频和视频。$$Output = Program(InputFeatures)$$另一个 计算机科学的措辞是将数据或观察的一行称为一个实例（instance）。这是因为一行可能被视为是由问题域观察或生成的单个样例或单个数据实例。$$Prediction = Program(Instance)$$ 模型和算法最后有一个重要的说明需要强调下，那就是算法和模型的关系。这可能会令人很困惑，因为算法和模型可能互换使用。我喜欢的一个观点是，将模型视为从数据中学习得具体表示，将算法视为学习它的过程。$$Model = Algorithm(Data)$$例如，一个决策树或一组系数是一个模型，C5.0和最小二乘线性回归（Least Squares Linear Regression）是学习那些模型的算法。 总结在本章中，你学到了在机器学习中用于描述数据的关键术语。对表格数据有了标准理解，如列、行、单元格；学习到了输入和输出变量的统计学术语，其分别表示为X和Y；学习到了计算机科学术语：属性、特征和实例；最后，学习到了可以将模型和算法分别看作是学习表示和学习过程。 现在你已经知道了如何在机器学习中讨论数据。接下来的章节中，你将会学到所有机器学习算法的基础范例。]]></content>
      <categories>
        <category>Master MLA</category>
      </categories>
      <tags>
        <tag>Data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter 01 Welcome]]></title>
    <url>%2F2018%2F06%2F04%2FMasterMLA-Ch-01%2F</url>
    <content type="text"><![CDATA[此为 Master Machine Learning Algorithms 一书的译文记录。欢迎阅读Master Machine Learning Algorithms。本书将从头开始教你10个强大的机器学习算法。 开发人员可通过阅读算法理论描述和应用实际案例来达到学习的最佳效果（理论+应用）。本书主要用来指导开发人员学习机器学习算法。本书结构包含有机器学习算法的程序性描述以及展示如何将数值运用到各个等式中和等式另一边输出怎样数值的按步教程。阅读完本书的算法描述和教程后，你将能够： 理解并解释先进机器学习算法的工作原理 用编程语言或工具实现算法原型 本书是对机器学习算法内部的一个导览。 读者本书是为开发人员所编写的。它并不承担统计学、概率论或线性代数的背景知识介绍。如果你了解一些概率统计的知识，那将会有所帮助，因为我们将会讨论诸如平均值、标准差和高斯分布等概念。如果你已对这方面内容感到生疏或者不确定，不要担心，你会看到对应的公式和应用示例以便能够将它们结合在一起。 本书也不承担机器学习的背景知识介绍。如果你了解其中的相关信息，会有所帮助，但是本书的目标是从头开始学习机器学习算法。具体而言，我们关注的是，当我们构建模型以对新数据做出预测（称为预测建模）时所选用的机器学习类型。如果这对于你来说是新的内容，不要担心，我们很快就会介绍机器学习算法的详细内容。 最后，本书假定你并不知道如何编码或编码能力较弱。你可以按照表格中的示例进行操作。事实上，我们强烈建议你能按照表格中内容去学习。如果你是程序员，可以将示例用你最喜欢的编程语言进行编写，这可作为你学习过程的一部分。 算法描述本书中对算法的描述和展示是经过精心编排的。每种算法将会根据这三种关键属性进行描述： 算法根据可存储在文件中的实际数字和结构而展示的表现形式 算法从训练数据中学习的过程 算法根据给定学习模型做出预测的过程 本书中会涉及到较少的数学。包含这些公式的原因是它们是获取目标想法的最佳方式。每个公式也会尽可能地用文字进行描述，并将提供一个应用示例来向你展示该如何运用它。 最后，也是最重要的，本书中介绍的每个算法都将包含一个按步教程。这样你就可以看出学习和预测过程是如何与真实数值一起运行的。每个教程都提供了充足的细节，能让你可根据表格或选择的编程语言进行操作。其中包括原始输入数据和包含精度值的每个公式的输出。 全书结构本书分为四个部分： 机器学习算法的背景 线性机器学习算法 非线性机器学习算法 集成机器学习算法 算法背景本部分将会展示机器学习算法的基础。它会教你所有的机器学习算法是如何连接的，并试图解决相同的底层问题。这部分会给你上下文，以便能理解任何机器学习算法。你会学习到： 机器学习应用中描述数据的术语 理解由所有机器学习算法解决的问题的框架 参数和非参数算法之间的重要区别 监督、无监督和半监督机器学习问题之间的对比 由偏差和方差引入的误差是这些关注点的权衡 应用机器学习方法解决过拟合数据问题 线性算法这部分将会从简单线性算法开始，带你融入进机器学习算法。这些可能是简单的算法，但它们也是理解更强大技术的重要基础。你将会看到如下所示的线性算法： 梯度下降优化程序，可用作许多机器学习算法的核心 线性回归，书中使用两个教程来预测真实值，确保你能沉浸其中 逻辑回归，对两类问题进行分类 线性判别分析，用于两类以上问题的分类 非线性算法这部分将介绍基于线性算法的更强大的非线性机器学习算法。这些技术可以减少对你问题的假设，能够学习各种各样的问题类型。但是，这种能力需谨慎使用，因为它们可以学习得太好，以致于过拟合你的训练数据。你将会学习到以下非线性算法： 分类和回归树是主要决策树算法 朴素贝叶斯使用概率来进行分类，我们通过两个教程向你展示该方法的有效使用方式 K近邻，除了数据集以外不需要任何模型 矢量化，通过学习在尺寸size上压缩你的训练数据集来扩展K近邻 支持向量机，可能是最流行和最强大的现成算法之一 集成学习集成学习是一种功能强大且更先进的机器学习算法类型。这种技术结合了多个模型的预测，以提供更为准确的预测。在本部分中，你将会看到两个最常用的集成学习方法： Bagging和Random Forests是最强大的算法之一 Boosting集成和AdaBoost算法依次修正了较弱模型的预测 本书不是什么本书不是机器学习的教科书。我们不会深入探讨事物为什么起作用的原理或公式的推导。本书旨在教学机器学习算法如何工作，而不是它们为什么工作。 本书不是机器学习编程书。我们不会为生产或操作使用而设计机器学习算法。本书中所有的样例仅用于演示目的。 如何使用本书本书旨在从一端到另一端线性阅读。当然仅阅读本书是不够的。为了深入理解概念和学习机器学习算法，你需要完成书中的教程。如果你在本书的旁边再打开表格查看教程，你将会获得最大的收益。 通过对教程的学习，你将会了解到对每个算法描述的表示、学习和预测过程的背景信息。在那里，你可以将你的想法转化为自己的程序，以及在实践中应用这些算法。 建议每天完成一章节，建议晚上坐在电脑前学习，这样就能立即尝试所学到的东西。我有意重复了关键的公式和描述，这样你能够快速回忆起之前学的内容。 总结本书是机器学习算法的入场券。接下来，你将构建一个基础来了解所有机器学习算法试图解决的基本问题。]]></content>
      <categories>
        <category>Master MLA</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[六一，你懂的]]></title>
    <url>%2F2018%2F06%2F01%2F20180601%2F</url>
    <content type="text"><![CDATA[哈哈，想着今天是六一儿童节，我也就跟着一波风。祝各位大朋友天天开心哈！]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo NexT主题内添加相册功能]]></title>
    <url>%2F2018%2F05%2F26%2Fnext-add-photos%2F</url>
    <content type="text"><![CDATA[给博客添加一个相册页面，以展示自己拍摄的一些照片 (≖ᴗ≖)✧ _config.yml首先新建hexo new page photos相册页面，将会在source/下创建photos/index.md，在其中添加type: photos 之后在主题_config.yml文件中对应位置menu里添加Photos: /photos/ || image ，这样生成后就能在页面的对应页面选项中有该相册Tab。 json在博客根目录下新建uploadPhotos文件夹，里面将会存放照片以及相关js文件。 新建uploadPhotos/Images/文件夹，然后在其中存放需要在页面中展示的照片集（后续需将该文件夹上传至GitHub相册库，若没有对应库，可新建，并上传图片）。 新建uploadPhotos/tool.js文件，里面内容如下，主要功能是访问照片文件夹，获取每张照片的size和name，并生成对应的json文件： 命令：Git Bash中键入 node tool.js生成json注：若出现Error: Cannot find module &#39;image-size&#39;问题，请在Git Bash中键入对应命令npm install image-size进行安装。12345678910111213141516171819202122232425262728293031&quot;use strict&quot;; const fs = require(&quot;fs&quot;); const sizeOf = require(&apos;image-size&apos;); const path = &quot;Images&quot;; const output = &quot;../themes/next/source/photos/photoslist.json&quot;; var dimensions; fs.readdir(path, function (err, files) &#123; if (err) &#123; return; &#125; let arr = []; (function iterator(index) &#123; if (index == files.length) &#123; fs.writeFile(output, JSON.stringify(arr, null, &quot;\t&quot;)); return; &#125; fs.stat(path + &quot;/&quot; + files[index], function (err, stats) &#123; if (err) &#123; return; &#125; if (stats.isFile()) &#123; dimensions = sizeOf(path + &quot;/&quot; + files[index]); console.log(dimensions.width, dimensions.height); arr.push(dimensions.width + &apos;.&apos; + dimensions.height + &apos; &apos; + files[index]); &#125; iterator(index + 1); &#125;) &#125;(0)); &#125;); json文件样例如下：123456789[ &quot;4032.3024 IMG_0391.JPG&quot;, &quot;12500.3874 IMG_0404.JPG&quot;, &quot;4032.3024 IMG_0416.JPG&quot;, &quot;4032.3024 IMG_0424.JPG&quot;, &quot;3024.4032 IMG_0427.JPG&quot;, &quot;4032.3024 IMG_0429.JPG&quot;, &quot;4032.3024 IMG_0430.JPG&quot;] photo.js新建themes/next/source/photos/photo.js文件，其中photos文件夹是自己创建的。 photos.js内容如下，主要功能是访问json文件内容，遍历每行数据，并在页面对应位置上放置代码，展示图片（其中图片链接为自个GitHub相册库中图片的链接）：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253photo =&#123; page: 1, offset: 20, init: function () &#123; var that = this; $.getJSON(&quot;/photos/photoslist.json&quot;, function (data) &#123; that.render(that.page, data); //that.scroll(data); &#125;); &#125;, render: function (page, data) &#123; var begin = (page - 1) * this.offset; var end = page * this.offset; if (begin &gt;= data.length) return; var html, imgNameWithPattern, imgName, imageSize, imageX, imageY, li = &quot;&quot;; for (var i = begin; i &lt; end &amp;&amp; i &lt; data.length; i++) &#123; imgNameWithPattern = data[i].split(&apos; &apos;)[1]; imgName = imgNameWithPattern.split(&apos;.&apos;)[0] imageSize = data[i].split(&apos; &apos;)[0]; imageX = imageSize.split(&apos;.&apos;)[0]; imageY = imageSize.split(&apos;.&apos;)[1]; li += &apos;&lt;div class=&quot;card&quot; style=&quot;width:330px&quot;&gt;&apos; + &apos;&lt;div class=&quot;ImageInCard&quot; style=&quot;height:&apos;+ 330 * imageY / imageX + &apos;px&quot;&gt;&apos; + &apos;&lt;a data-fancybox=&quot;gallery&quot; href=&quot;https://github.com/asdfv1929/BlogPhotos/blob/master/Images/&apos; + imgNameWithPattern + &apos;?raw=true&quot; data-caption=&quot;&apos; + imgName + &apos;&quot;&gt;&apos; + &apos;&lt;img src=&quot;https://github.com/asdfv1929/BlogPhotos/blob/master/Images/&apos; + imgNameWithPattern + &apos;?raw=true&quot;/&gt;&apos; + &apos;&lt;/a&gt;&apos; + &apos;&lt;/div&gt;&apos; + // &apos;&lt;div class=&quot;TextInCard&quot;&gt;&apos; + imgName + &apos;&lt;/div&gt;&apos; + &apos;&lt;/div&gt;&apos; &#125; $(&quot;.ImageGrid&quot;).append(li); $(&quot;.ImageGrid&quot;).lazyload(); this.minigrid(); &#125;, minigrid: function() &#123; var grid = new Minigrid(&#123; container: &apos;.ImageGrid&apos;, item: &apos;.card&apos;, gutter: 12 &#125;); grid.mount(); $(window).resize(function() &#123; grid.mount(); &#125;); &#125;&#125;photo.init(); photos.swig新建themes/next/layout/photos.swig文件，其内容模仿tag.swig中内容（直接复制粘贴），然后将其中的tag全部替换为photos。 photos.css新建themes/next/source/photos/photos.css样式文件，内容如下：12345.ImageGrid &#123;width: 100%;max-width: 1040px;margin: 0 auto; text-align: center;&#125;.card &#123;overflow: hidden;transition: .3s ease-in-out; border-radius: 8px; background-color: #ddd;&#125;.ImageInCard &#123;&#125;.ImageInCard img &#123;padding: 0 0 0 0;&#125;.TextInCard &#123;line-height: 54px; background-color: #ffffff; font-size: 24px;&#125; page.swig修改themes/next/layout/page.swig文件，添加下面代码中中间page.type === &quot;photos&quot;那两行。1234567891011121314151617&#123;% block title %&#125;&#123;##&#125;&#123;% set page_title_suffix = &apos; | &apos; + config.title %&#125;&#123;##&#125;&#123;% if page.type === &quot;categories&quot; and not page.title %&#125;&#123;# #&#125;&#123;&#123; __(&apos;title.category&apos;) + page_title_suffix &#125;&#125;&#123;##&#125;&#123;% elif page.type === &quot;tags&quot; and not page.title %&#125;&#123;# #&#125;&#123;&#123; __(&apos;title.tag&apos;) + page_title_suffix &#125;&#125;&#123;##&#125;&#123;% elif page.type === &quot;photos&quot; and not page.title %&#125;&#123;# #&#125;&#123;&#123; __(&apos;title.photos&apos;) + page_title_suffix &#125;&#125;&#123;##&#125;&#123;% else %&#125;&#123;# #&#125;&#123;&#123; page.title + page_title_suffix &#125;&#125;&#123;##&#125;&#123;% endif %&#125;&#123;##&#125;&#123;% endblock %&#125; 依旧是该文件中，添加page.type === &quot;photos&quot;那两行：123456789101112131415161718&#123;% elif page.type === &apos;categories&apos; %&#125; &lt;div class=&quot;category-all-page&quot;&gt; &lt;div class=&quot;category-all-title&quot;&gt; &#123;&#123; _p(&apos;counter.categories&apos;, site.categories.length) &#125;&#125; &lt;/div&gt; &lt;div class=&quot;category-all&quot;&gt; &#123;&#123; list_categories() &#125;&#125; &lt;/div&gt; &lt;/div&gt;&#123;% elif page.type === &apos;photos&apos; %&#125; &lt;div class=&quot;ImageGrid&quot;&gt;&lt;/div&gt;&#123;% else %&#125; &#123;&#123; page.content &#125;&#125;&#123;% endif %&#125; _config.yml在主题配置文件_config.yml中相关部分，进行相关的配置（lazyload、fancybox）：12345678910111213# Fancyboxfancybox: true# Internal version: 2.1.5# See: http://fancyapps.com/fancybox/fancybox: https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.3.5/jquery.fancybox.min.jsfancybox_css: https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.3.5/jquery.fancybox.min.css# Internal version: 1.9.7# See: https://github.com/tuupola/jquery_lazyloadlazyload: https://cdn.jsdelivr.net/npm/lazyload@2.0.0-beta.2/lazyload.js _layout.swig中相关信息配置：1234&lt;script type=&quot;text/javascript&quot; src=&quot;https://unpkg.com/minigrid@3.1.1/dist/minigrid.min.js&quot;&gt;&lt;/script&gt;&lt;link rel=&quot;stylesheet&quot; href=&quot;/photos/photos.css&quot;&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/photos/photo.js&quot;&gt;&lt;/script&gt; 重新生成访问，查看成果。 总结以上就是添加相册功能大概流程，因为步骤比较多，且是通过后期回忆步骤进行记录，所以可能存在些许问题，还请原谅，并请把出现的问题在本文下面的评论中点出，我会进行修改。 后续的实现： 将照片上传至GitHub相册库时，由于照片分辨率较高，其都达到了两三M以上，上传速度较慢，导致上传进度缓慢。后期想通过代码将照片进行压缩后再上传至相册库。 相册展示整个操作流程为：先上传照片到git库，再生成json文件，之后便是正常的clean、g、d，后期想把压缩、上传照片和生成json文件整合到一起。 目前的照片展示都是所有照片一整块放一起进行瀑布流显示，后期想将照片根据其旅游场景或类别、时间不同进行分类至对应文件夹，并根据类别或时间线显式展示出不同文件夹下的照片。 参考链接：hexo主题中添加相册功能如何在Hexo中实现自适应响应式相册功能]]></content>
      <categories>
        <category>GitBlog</category>
      </categories>
      <tags>
        <tag>Photos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo NexT主题中添加网页音乐播放器功能]]></title>
    <url>%2F2018%2F05%2F26%2Fnext-add-music%2F</url>
    <content type="text"><![CDATA[为博客添加网页音乐播放器功能 download点击访问Aplayer源码：GitHub Aplayer。下载到本地，解压后将dist文件夹复制到themes\next\source文件夹下。 music.js新建themes\next\source\dist\music.js文件，添加内容：12345678910111213141516171819202122232425const ap = new APlayer(&#123; container: document.getElementById(&apos;aplayer&apos;), fixed: true, autoplay: false, audio: [ &#123; name: &quot;PDD洪荒之力&quot;, artist: &apos;徐梦圆&apos;, url: &apos;http://up.mcyt.net/?down/39868.mp3&apos;, cover: &apos;http://oeff2vktt.bkt.clouddn.com/image/84.jpg&apos;, &#125;, &#123; name: &apos;9420&apos;, artist: &apos;麦小兜&apos;, url: &apos;http://up.mcyt.net/?down/45967.mp3&apos;, cover: &apos;http://oeff2vktt.bkt.clouddn.com/image/8.jpg&apos;, &#125;, &#123; name: &apos;风筝误&apos;, artist: &apos;刘珂矣&apos;, url: &apos;http://up.mcyt.net/?down/46644.mp3&apos;, cover: &apos;http://oeff2vktt.bkt.clouddn.com/image/96.jpg&apos;, &#125; ]&#125;); 源码中对应的参数解释，这边都有： Aplayer 中文文档 audio对应的便是音频文件，所以音乐播放器需要播放的音乐是需要自己进行相关信息（如歌曲链接、歌词、封面等）的配置。这里放一个mp3音乐外链网站：http://up.mcyt.net/ ，搜索对应的音乐，然后复制url和右击封面图片链接粘贴到对应的位置上就行了。 注：由于该外链网站没有歌词链接，我这边没有进行配置，所以播放器在播放音乐时点击歌词是没有显示的。 _layout.swig打开themes\next\layout\_layout.swig文件，将1234&lt;link rel=&quot;stylesheet&quot; href=&quot;/dist/APlayer.min.css&quot;&gt;&lt;div id=&quot;aplayer&quot;&gt;&lt;/div&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/dist/APlayer.min.js&quot;&gt;&lt;/script&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/dist/music.js&quot;&gt;&lt;/script&gt; 添加到&lt;body itemscope ...&gt;后面就行，即在&lt;body&gt;&lt;/body&gt;里面。 重新生成，访问页面，就能看到左下角的音乐播放器了。]]></content>
      <categories>
        <category>GitBlog</category>
      </categories>
      <tags>
        <tag>Web Music</tag>
        <tag>Aplayer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo NexT主题中添加百度分享功能]]></title>
    <url>%2F2018%2F05%2F25%2Fbaidu-share%2F</url>
    <content type="text"><![CDATA[给博文增加简单的百度分享功能 _config.yml因为next\layout_partials\share\baidushare.swig文件中代码显示：12345&#123;% if theme.baidushare.type === &quot;button&quot; %&#125;......&#123;% elseif theme.baidushare.type === &quot;slide&quot; %&#125;... 在配置百度分享功能时需指定其type，所以将主题配置_config.yml文件中关于baidushare部分的内容改为（其中type亦可以选择button）：123baidushare: type: slide baidushare: true download_config.yml中提示：Warning: Baidu Share does not support https.因为百度分享不支持在https上使用，所以一种解决方法便是，直接放文件到我们自己的目录下面。 访问链接： static文件夹下载压缩包到本地，解压后，将static文件夹保存至themes\next\source目录下。 baidushare.swig修改文件：themes\next\layout_partials\share\baidushare.swig将文件 末尾 部分的代码进行修改：1.src=&apos;http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion=&apos;+~(-new Date()/36e5)]; 改为1.src=&apos;/static/api/js/share.js?v=89860593.js?cdnversion=&apos;+~(-new Date()/36e5)]; 最后重新生成下，就能展示分享功能了。 参考链接：百度分享不支持https的解决方案Hexo+Github搭建个人博客(三)——百度分享集成]]></content>
      <categories>
        <category>GitBlog</category>
      </categories>
      <tags>
        <tag>NexT</tag>
        <tag>BaiduShare</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Eclipse Java 快捷键记录]]></title>
    <url>%2F2018%2F05%2F21%2Feclipse-java-hotkey%2F</url>
    <content type="text"><![CDATA[记录一下Eclipse中相关的快捷键。 Eclipse的快捷键 a: 内容辅助键 Alt + / 自动补齐main方法 main 然后 Alt + / 自动补齐输出语句 syso 然后 Alt + / b: 格式化代码 Ctrl + Shift + f 代码区域右键 – Source – Format c: 自动导包 Ctrl + Shift + o 如果当前类在多个包中都存在，这时候，使用 Ctrl + shift + o ,进行选择一个包导入即可。 d: 注释 单行注释 加注释： 先选中需要注释的内容，然后 Ctrl + / 取消注释：先选中需要取消注释的内容， 然后 Ctrl + / 多行注释 加注释： 先选中需要注释的内容，然后 Ctrl + Shift + / 取消注释：先选中需要取消注释的内容， 然后 Ctrl + Shift + \ e: 补充 代码上下移动 选中代码 alt + 上/下箭头 查看源码 选中类名(F3或者Ctrl + 鼠标点击) 查找具体的类 ctrl + shift + t，输入要查找的类的名称–&gt;确定 查找具体类的具体方法 ctrl + o 给建议 ctrl + 1,根据右边生成左边的数据类型,生成方法 删除代码 ctrl + d 抽取方法 alt + shift + m 改名 alt + shift + r（类名，方法名，变量名）]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Eclipse</tag>
        <tag>Hot Key</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图像显著性检测 LC算法]]></title>
    <url>%2F2018%2F05%2F11%2Fsaliency-LC%2F</url>
    <content type="text"><![CDATA[图像显著性检测算法之一：LC Algorithm 基本思想计算某个像素在整个图像上的全局对比度，即该像素与图像中其他所有像素在颜色上的距离之和作为该像素的显著值。 显著值计算图像 $I$ 中某个像素 $I_k$ 的显著值计算如下：其中$I_i$的取值范围为 $[0, 255]$， 即为灰度值。 上式等于：$N$表示图像中像素的数量。 给定一张图像，每个像素$I_k$的颜色值已知。假定$I_k = a_m$，则上式可进一步重构：其中，$f_n$表示图像中第$n$个像素的频数，以直方图的形式表示。 代码实现直接调用OpenCV接口，实现图像中像素的直方图统计，即统计$[0, 255]$中每个灰度值的数量。12# 直方图，统计图像中每个灰度值的数量hist_array = cv2.calcHist([image_gray], [0], None, [256], [0.0, 256.0]) 计算像素与其他所有像素在灰度值上的距离。12345678def cal_dist(hist): dist = &#123;&#125; for gray in range(256): value = 0.0 for k in range(256): value += hist[k][0] * abs(gray - k) dist[gray] = value return dist 将灰度值图像中的像素值更新为对比度值（即距离度量）。12345for i in range(image_width): for j in range(image_height): temp = image_gray[j][i] image_gray_copy[j][i] = gray_dist[temp]image_gray_copy = (image_gray_copy - np.min(image_gray_copy)) / (np.max(image_gray_copy) - np.min(image_gray_copy)) 代码链接： https://github.com/asdfv1929/Saliency_LC_Algorithm 结果展示原图： 显著图：]]></content>
      <categories>
        <category>Saliency Map</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Image Processing</tag>
        <tag>Saliency</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文献管理工具]]></title>
    <url>%2F2018%2F03%2F29%2FZotero%2F</url>
    <content type="text"><![CDATA[推荐一个文献管理工具：Zotero链接走起 &gt;&gt;&gt;&gt;&gt;&gt;&gt; Zotero 这边我简单地走一下软件的使用流程（因为我自己也只是会简单的应用，尴尬(*￣︶￣) ） 安装安装软件我就不说了，就一般程序安装步骤。在安装的过程中，Zotero会在本地的Word程序中安装插件，所以会有这样子的Tab出现： 添加文献信息比如添加一篇文献，可以先进入百度学术，搜索对应的文献，这边我搜索的是 deep learning，然后点击“引用” 之后点击“BibTex”，页面会跳转到显示如下文本内容： Ctrl+A Ctrl+C 就是复制一下该页文本。然后打开Zotero软件，点击“从剪贴板导入”，或者直接快捷键“Ctrl+Shift+Alt+I”，这样，该文献的信息就被导入进Zotero中了。 插入文档在文档Word中插入文献信息，就是在这个Tab中操作的。首先鼠标点击到文档中目标位置，然后点击“Add/Edit Citation” 会有Zotero的弹出框出来，在这里选择经典视图：之后便会出现如下所示的文献选取框，在这里就选定目标文献，点击OK就行了，目标位置处会有对应的文献序号索引生成，我就不截图了。 之后在论文中的参考文献章节下，鼠标移动到这，然后点击Tab中的Add/Edit Bibliography，就会在该鼠标处自动生成对应的文献信息：Lecun Y, Bengio Y, Hinton G. Deep learning[J]. Nature, 2015, 521(7553): 436. 之后的其他文献插入操作类似，都是这样进行，且文献序号是自动生成的，从前往后，后面的参考文献依次对应。 关于Zotero的一些操作，我就大体讲这些，，后续的相关操作就靠大家自己摸索了。 全军出击(((ꎤ’ω’)و三 ꎤ’ω’)-o≡]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Paper</tag>
        <tag>Tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译：Foreground Object Sensing for Saliency Detection]]></title>
    <url>%2F2018%2F03%2F22%2FForeground-Object-Sensing-for-Saliency-Detection%2F</url>
    <content type="text"><![CDATA[主要是关于图像显著性检测的一篇论文。 Name DESC TITLE Foreground Object Sensing for Saliency Detection AUTHOR Hengliang Zhu, Bin Sheng, Xiao Lin, Yangyang Hao, Lizhuang Ma PBYEAR 2016 TRANSLATION asdfv1929 TIME 2018.3 摘要许多先进的显著性检测算法都依赖于边界先验（boundary prior），但是这些算法只是简单地将图像周围的边界视为背景区域。在这里，我们提出一种快速有效的显著目标检测算法。首先提出一个新的方法，通过使用Harris Corner的凸包（convex hull）来近似定位前景目标。在此基础上，我们将不同区域的显著值划分为两部分，生成对应的贴图（cue maps）（前景和背景），并将其合并为凸包先验图（a convex hull prior map）。然后基于到凸包中心的距离提出一个新的先验来代替之前的中心先验。最后，将凸包先验图和凸包中心偏差（center-biased）图组合为显著图，然后对显著图进行优化以获得最终结果。通过与18个现有算法进行比较，在几个数据集上进行测试，本算法在精度和查全率（precision and recall）方面均表现良好。CCS概念：计算方法 –&gt; 兴趣点和显著区域检测关键字：前景目标；Harris Corner；凸包；显著图 引言在对生物（人类）视觉系统的信息处理机制进行研究之后，近年来关于视觉显著性检测的工作迅速发展增加，并成功地应用到众多计算机视觉应用中，如图像分割[34]，图像压缩[13, 23]，图像缩放[29]，图像检索[8, 35]和图像质量评估[46]。“视觉显著性”的目标是找到图像中的丰富信息区域（abundant information regions），其中的显著区域和非显著区域在感知上是可区分的。 从心理学角度[22]来看，显著性检测可以分为自下而上（数据驱动）方法[7, 12, 15, 32, 39, 45, 48]和自上而下（目标驱动）方法[20, 27, 47]。与后者利用图像的高级先验知识相比，前者利用图像的低级特征（例如，颜色、位置和纹理）来检测显著性区域。有了这些原始特征，在之前的工作[1, 18, 28, 37, 48]中，研究者提出了诸如中心先验（center prior）、对比度先验（contrast prior）和边界先验（boundary prior）等先验假设，以提高显著目标检测的准确性。在本文中，我们主要关注自下而上的显著性检测方法。 近年来，显著性检测通常采用图像边界先验，它假定图像的窄边界是非显著性区域。换句话说，图像边界倾向于被视为是背景。有许多先进的算法依赖于边界先验[19, 41, 45, 48]，并且获得了更好的性能。然而，这些工作的一个局限性便是显著目标可能会轻微触碰到图像边缘。换句话说，检测到的显著目标在图像边界区域是不完整的。 为解决这个问题，我们提出了一个简单有效的显著区域检测算法（工作流程见图1）。首先，基于角点检测算法（Harris Corner[40]），提出一个新的前景目标定位算法，该算法使用两个凸包交叉区域（intersections）来近似定位前景目标。交叉区域排除了更多分散的背景，并集中在显著目标的周围。其次，通过结合全局对比度线索（global contrast cues），我们引入了一个新的区域（图像块）显著性计算阶段（详见Section 3）。我们将一个区域的显著值划分为两部分：一部分位于凸包内部（显著区域，可能包含一些背景），另一部分位于凸包外部（背景区域）。它们被用于生成两个贴图（cue maps）（前景和背景），这两个贴图之后组合成凸包先验图。最后，优化得到显著图。 文章剩余部分安排如下。Section 2回顾了相关工作。所提出的显著性计算方法的细节在Section 3和Section 4中给出。Section 5展示实验结果和讨论。总结在Section 6中给出。 相关工作[4, 5]中提供了显著性检测的详细总结。跟踪眼球移动的固定预测模型[3, 11, 14, 16]主要是利用到物理设备。相反，这项工作是基于图像处理的研究。本节中，我们简要回顾下主要的显著性检测算法，还有它们之间的区别。 关于显著性检测的开创性研究是由Itti等人所进行的[17]，他们将像素的显著值定义为局部中心-环绕差异（local center-surround difference）。该检测基于多尺寸的图像特征，包括颜色、强度和方向。 之后，包括局部或全局对比的对比先验（contrast prior）被广泛用于检测显著目标和区域。例如，Ma等人[28]提出了基于局部对比的显著区域检测方法，该方法以单一尺寸运行。Cheng等人[7]在显著性计算时考虑了全局对比。Perazzi等人[32]使用唯一性和空间分布来计算显著图。Goferman等人[12]提出了一个情景感知（context-aware）显著性检测模型。Achanta等人[1]提出了一种多尺寸方法来生成显著图。所有这些方法都利用图像的低级特征来表征（characterize ）视觉显著性。另外，一些工作[19, 20, 27]以机器学习的方式使用了原始特征，并且产生了更优的结果。 最近，边界先验被应用于检测显著区域，其中图像边界区域被简单地视为背景。直观地说，显著区域与图像边界的连接要比背景中的显著区域少得多。Wei等人[41]提出了一种测地（geodesic）显著性方法，它将背景视为线索提示（cues）而不是显著目标。Chuan Yang等人[45]提出了一种基于图形的流形（manifold）排序方法，它使用图像背景区域作为查询。W.Zhu等人[48]提出了一种称为边界连通性的显著性度量，它利用图像块与图像边界之间的空间关系。X.Li等人[24]通过使用背景模板来测量密集和稀疏重构建错误（dense and sparse reconstruction errors）的显著性。为了利用区域与其邻居的内在联系，Q.Yao等人[33]将显著目标检测视为细胞自动演化过程。但从另一个角度来看，显著性检测的另一个关键点是前景对象的位置。Harris点的凸包被广泛用于近似估计前景区域。例如，可以使用基于凸包的低级提示来估计显著性[42, 43]。R.Liu等人[26]采用Harris算子来获得候选前景和背景。Pan等人[31]通过使用图像抠像模型（image matting model）提出了一种边缘保留滤波（edge preserving filter）方法。 然而，边界先验算法只是将图像边界视为背景，而基于凸包的算法可能包含更多非显著区域。为了克服这些困难，我们提出了用于显著性检测的前景目标感知度量（foreground object sensing measure）。 前景目标感知算法步骤如下。首先，我们提出一种新方法来近似定位前景目标。之后，生成两个显著贴图（前景和背景），并最终以简单的方式将其集成到凸包先验图中。然后，我们提出一种基于凸包中心偏差的加权全局对比算法。这些详细内容分别在Section 3.1-3.3中讨论。 目标位置估计根据视觉注意机制，许多研究均表明，显著区域有一个明显不同的外观，并可以与其周围区分开来。受这一原则启发，我们使用图像的特征点来获取凸包，然后大致定位前景目标。 现有许多特征点检测器，如scale-invariant feature transform（SIFT），speeded-up robust features（SURF）和Harris。考虑到计算复杂度低，我们利用Harris Corner来粗略检测显著目标的特征点。在实验中，我们发现Harris角点只用于计算显著区域的凸包是不恰当的，因为凸包区域可能包含了太多的背景。为了更准确地定位前景目标，我们需要进一步消除凸包区域中的背景影响。 通常，显著目标是图像中信息最丰富的区域，其边缘或轮廓在像素上的强度或颜色是不连续的。然而，在频域（frequency domain）中，亮度变化可能对应着高频分量。因此，为了抑制属于背景的一些高频成分，我们使用低通滤波器来平滑原始图像，例如平均滤波器和高斯低通滤波器。然后我们再次使用Harris算子来检测滤波图像中的角点并获得相应的凸包。两个凸包区域分别表示为$R_1$和$R_2$（图2(b)，图2(c)），然后确定它们的交点。我们将这个交叉区域定义为$$\begin{equation}R_F = R_1 \cap R_2\end{equation}$$图2显示了使用凸包的前景目标区域的定位。交叉区域（红色多边形）排除了更多分散的背景（图2(d)），并且可以近似视为前景目标区域。此外，图像的背景（$R_F$外部）区域被表示为$R_B$。在我们的实验中，我们使用平均滤波器，将其大小设置为35。 显著性计算在计算显著图之前，我们先使用简单线性迭代聚类算法（SLIC）[2]将原始图像分割为超像素（superpixels）区域，这样可以有效地生成紧凑和均匀的超像素。在我们的实验中，我们将图像中超像素的数量设置为300。超像素区域集合表示为$X = {r_1,r_2,…,r_N}$，其对应地显著值为$V = {v_1,v_2,…,v_N}$，其中$v_i$是超像素区域$r_i$的显著值，$N$是超像素的数量。 基于前景目标的位置，我们将每个超像素的显著值分为两个部分：一部分在凸包内部，一部分在凸包外部，分别表示为$SF_i$和$SB_i$。然后我们通过连接所有相邻的超像素$r_i$和$r_j$来构造一个无向图，图中边的权重是CIELab颜色空间中超像素的平均颜色之间的欧氏距离。因此，基于该图，我们使用测地距离（geodesic distance）来计算两个超像素区域之间的差异，这很容易通过最短路径算法计算得到。 因此，我们将超像素区域的$SF_i$定义为与$R_F$中的所有区域相比测地距离的总和。所以，$SF_i$写为：$$\begin{equation}SF_i = \sum_{j=1}^{N_1}shtDist(c_i, c_j), r_j \in R_F\end{equation}$$其中$shtDist(c_i, c_j)$是两区域$r_i$和$r_j$之间的最短路径；$c_i$和$c_j$分别是CIELab颜色空间中超像素$r_i$和$r_j$的平均颜色，区域$r_j$属于$R_F$;$N_1$是凸包内超像素的总共数量。样例如图3(b)所示。 与$SF_i$一样，我们将超像素区域的$SB_i$定义为与$R_B$中所有区域相比的测地距离的总和。因此，$SB_i$由下式计算得到：$$\begin{equation}SB_i = \sum_{j=1}^{N_2}shtDist(c_i, c_j), r_j \in R_B\end{equation}$$其中$r_j$属于$R_B$；$N_2$是凸包外部超像素的总数，如图3(c)所示。注意，$N = N_1 + N_2$。 前景贴图（Foreground Cue Map） 显著区域通常是重要且吸引人的地方[5]，并且通常在与周围环境中比较突出。因此，$R_F$中超像素区域的显著值要高于$R_B$中。对于给定区域$r_i$，如果它在凸包内，则$SF_i$小于$SB_i$，所以$SB_i / SF_i$高；如果它位于凸包外部，则$SF_i$大于$SB_i$，所以$SB_i / SF_i$低。 鉴于这个事实，我们使用凸包区域构造前景贴图。图像中每个超像素区域的显著值定义为：$$\begin{equation}FC_i = SB_i / SF_i\end{equation}$$上述等式确保前景$R_F$中的区域的显著值高于背景$R_B$中的显著值。图4(b)展示了前景贴图的一些结果。显然，整个前景目标区域突出显示了，并且具有比凸包外部区域（背景）更大的值。 背景贴图（Background Cue Map） 与前景贴图相反，我们可以构造对应的背景贴图。背景$R_B$中的超像素区域拥有比前景区域超像素更大的显著值。背景贴图定义为：$$\begin{equation}BC_i = SF_i / SB_i\end{equation}$$其中$BC_i$是图像中一个区域的显著值。如图4(c)所示，我们可以看到背景区域几乎都是突出显示的。 凸包先验图（Convex Hull Prior Map） 前景贴图不够均匀，是因为一些背景区域具有高显著性（图4(b)），但背景贴图更突出地显示了非显著区域（图4(c)）。因此，我们提出了一种算法将这两个贴图组合成如下的凸包先验图：其中，$r_i$ 是超像素区域。事实上，背景贴图对背景有很大的影响，所以它的效果由参数 $\sigma_1$ 和 $\sigma_2$ 来平衡。当$r_i \in R_F$，我们分配给$\sigma_1$一个较小值；当$r_i \in R_B$，我们分配给$\sigma_2$一个较大值。注意，我们在实验中是将$\sigma_1$和$\sigma_2$设为常数值。根据经验，我们设置$\sigma_1 = 2.5$，$\sigma_2 = 6$。 此外，凸包先验图被归一化到范围[0, 1]内。根据一些视觉上的比较（图4(d)），背景区域明显减小。 凸包中心偏差算法（Convex hull center-biased algorithm）视觉注意研究显示人类倾向于将焦点集中在图像的中心区域[38]。有一些使用中心先验的显著性模型[18, 44]，认为靠近图像中心的区域可能是一个显著对象。换句话说，背景区域往往远离图像中心。然而，在许多情况下，中心先验模型可能无效并且错过了一些显著区域。因此，我们充分利用凸包中心偏差作为全局区域对比的加权因子[7]。假设$\mu(x_c, y_c)$表示凸包的中心，那么我们算法中区域的全局对比就写为：$$\begin{equation}G(r_i) = \sum_{j=1}^N \phi(r_i, r_j)\Vert c_i - c_j \Vert_2 \psi(r_i, \mu)\end{equation}$$其中$\phi(r_i, r_j)$是高斯权重函数，将其设置为$exp(-d(r_i, r_j)/(2\delta_1^2))$，表示$r_i$和$r_j$这两个区域中心之间的空间距离；$\psi(r_i, \mu)$是权重因子，将其设置为$exp(-d(r_i, \mu)/(2\delta_2^2))$；$d(r_i, \mu)$表示区域$r_i$和凸包的中心之间的欧氏距离。根据经验，$\delta_1$和$\delta_2$在我们的实验中分别设定为0.4和0.2。如图5所示，当前景目标不在图像中心时，第二和第三行出现了一些情况，图5(c)展示了我们算法的结果。 整合、优化凸包先验图和凸包中心偏差图互为补充。如图5(b)所示，我们可以看到前景目标明显突出显示，但仍有一些背景区域与显著目标没有明显分离开。 与前景目标相比，显著图中的背景区域被弱化（图5(c)）。因此，我们可以利用这两个显著图并以简单的方式将它们组合到最终的显著图中去。定义组合后的显著图为：$$\begin{equation}S_i = CH_i * G_i\end{equation}$$如组合显著图所示（图5(d)），我们可以看到，背景区域被上面等式给完全消除掉了。为了进一步生成均匀突出的显著映射，我们使用[48]中报告的优化函数来改进我们的结果。使用不同的系数，优化函数被重写为：$$\begin{equation}\sum_{i=1}^N B_iv_i^2 + \sum_{i=1}^N S_i(1-v_i)^2 + \sum_{i, j}\phi(r_i, r_j)(v_i, v_j)^2\end{equation}$$其中$v_i$是超像素的显著值，$B_i$由等式5和等式8决定，因为$B_i = (1 - S_i)BC_i$。系数$B_i$和$S_i$分别控制背景和前景显著值。第三项是平滑约束，它控制从前景到背景的平滑过渡（smooth transition）。图5(e)中展示了一些示例，显然，优化后显著目标很好地突出显示。 实验为了测试我们算法的性能，我们进行了两种实验：（1）性能评估，包括精确度和召回率（PR）曲线，F-度量和平均绝对误差（MAE），它们按照[15, 32]中描述的内容计算得到；（2）对所提出算法中对最终显著图做出贡献的每个分量做一些评估。 我们将我们的算法（表示为FOS）与现有的18种先进的显著性检测算法（包括有IT[17], FT[15], SR[16], CA[12], SF[32], GS[41], GC[10], HS[44], PCA[30], RC[7], SVO[6], LMLC[43], GMR[45], DSR[24], HDCT[21], wCO[48], BL[33], 和BSCA[39]）进行了比较。这些算法的结果是使用相应作者的公开可用源代码或demo来运行得到的。实验是在6个基准数据集（ASD[15], MSRA5000[27], THUS10000[9], ECSSD[44], Pascal-S[25] and SED2[36]）上测试进行的。 定量评估我们选择两种客观的评估方法（固定阈值和自适应阈值）来评估所有显著性检测算法。首先，对于每个显著图，使用位于[0, 255]范围内的增量为1的固定阈值来生成二值图。然后，我们使用精度和召回率的结果来与真实图进行比较。PR（precision-recall）曲线的结果如图6(a)和6(b)所示，其从上行到下行的结果分别为MSRA5000, THUS10000, ECSSD和Pascal-S。我们的算法在精度和召回率方面要优于其他算法。 其次，我们还估计每种算法的平均精确度、召回率、F-度量和MAE值。根据文献[15]中的配置，我们设置自适应阈值$T = 2*Mean(S) $ （$S$是显著映射），F-度量的表达式为： 其中我们设置$w^2 = 0.3$。这些值的结果展示在图6(c)条形图中。总之，与其他18种算法相比，我们的算法取得了良好的效果，特别是在F-度量和MAE方面。 为了准确评估这些显著性算法的性能，我们用5个数据集上的12个代表性算法提供了F-度量和MAE的定量结果（表1）。我们可以看到，我们的算法明显优于其他现有算法。 定性评估我们在图9中提供了本算法和其他十个先进的算法的显著映射。从结果中我们可以看出，我们的算法生成的显著映射可以清楚地将显著目标与背景分隔开。当图像具有复杂的背景结构时，我们的算法仍然以较少的噪声背景产生了有利的结果。例如，在第四行和第五行中，我们的显著图可以统一突出显示图象的前景目标，但其他算法无法从分散背景中提取突出目标。另外，当显著目标与背景具有相似的外观时，我们的算法能够精确地检测出显著区域，如第一行和第三行所示。通常，我们的算法可以生成具有突出显示显著目标的显著图，并有效地抑制背景区域（图9）。 算法分析为了证明我们所提出的算法的有效性，我们还评估了算法中每个分量的性能。每个分量都有助于最终的结果，如前景贴图，凸包先验图，凸包中心偏差图，组合显著图和最终显著图。图7显示了ASD（MSRA 1000）数据集上我们算法的每个单独分量的精度和召回率。很明显，凸包先验图和组合显著图也可以生成高精度结果。 限制我们还展示了在数据集SED2（100张图像）上我们算法和其他显著性检测器的实验结果。SED2包含有多个显著目标，这对显著性检测来说是一个巨大的挑战。结果如图8所示。我们可以看到，我们的算法在数据集SED2上的表现并不是最好的。如PR曲线（图8）所示，随着召回率的增加，精度不断下降。直观地说，当图像包含多个具有非常大空间距离的显著目标时，我们的算法在检测这些显著目标时存在一些限制，特别是在精度方面。由于图像中多个目标的大小和位置都是非常不同和多样化的，并且几乎不用我们算法中使用的一个凸包进行覆盖，如图8中的失败情况所示。也许我们可以使用多个凸包来提高多目标检测的算法能力。 时间复杂度我们在ASD数据集上比较了FOS和最具代表性或最新的算法。表2列出了每种算法的平均运行时间。我们使用相应作者的公开源代码来运行它们的算法，并且我们的算法是基于Matlab R2013a实现的。所有的实验都是在Intel Core i5-3470 CPU@3.2Hz 4G RAM、windows 8的PC上运行的。很明显，我们算法的计算复杂度较低。 总结本文中，我们通过前景目标感知提出了一种高效的显著性区域检测算法。为了准确定位前景目标，我们首先引入了一种新方法来近似估计显著目标的位置，这可以消除大部分背景信息。我们还提出了一个凸包中心偏差算法来提高对比前景区域并削弱背景影响。大量的实验结果表明，新算法可以生成高质量的均匀显著图，并均匀地突出显著区域。将来，我们将改进用于检测多个显著目标的算法。 文献[1] R. Achanta, F. Estrada, P. Wils, and S. Süsstrunk. Salient region detection and segmentation. Computer Vision Systems, pages 66–75, 2008.[2] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. Slic superpixels compared to state-of-the-art superpixel methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(11):2274–2282, Nov 2012.[3] B. Ali, D. N. Sihite, and I. Laurent. Quantitative analysis of human-model agreement in visual saliency modeling: a comparative study. IEEE Transactions on Image Processing, 22(1):55–69, 2013.[4] A. Borji, M.-M. Cheng, H. Jiang, and J. Li. Salient object detection: A survey. Eprint Arxiv, 16(7):3118, 2014.[5] A. Borji, D. N. Sihite, and L. Itti. Salient object detection: A benchmark. In European Conference on Computer Vision (ECCV), 2012.[6] K.-Y. Chang, T.-L. Liu, H.-T. Chen, and S.-H. Lai. Fusing generic objectness and visual saliency for salient object detection. In International Conference on Computer Vision (ICCV), pages 914–921, 2011.[7] M. Cheng, N. J. Mitra, X. Huang, P. H. S. Torr, and S. Hu. Global contrast based salient region detection. In Computer Vision and Pattern Recognition (CVPR), pages 409–416, 2011.[8] M.-M. Cheng, N. J. Mitra, X. Huang, and S.-M. Hu. Salientshape: Group saliency in image collections. The Visual Computer, pages 1–10, 2013.[9] M. M. Cheng, N. J. Mitra, X. Huang, P. H. S. Torr, and S. M. Hu. Salient object detection and segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(3):1, 2011.[10] M.-M. Cheng, J. Warrell, W.-Y. Lin, S. Zheng, V. Vineet, and N. Crook. Efficient salient region detection with soft image abstraction. In International Conference on Computer Vision (ICCV), pages 1529–1536, 2013.[11] D. Gao, V. Mahadevan, and N. Vasconcelos. The discriminant center-surround hypothesis for bottom-up saliency. Advances in Neural Information Processing Systems (NIPS), 20:497–504, 2007.[12] S. Goferman, L. Zelnik-Manor, and A. Tal. Context-aware saliency detection. In Computer Vision and Pattern Recognition (CVPR), pages 2376–2383, 2010.[13] C. Guo and L. Zhang. A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression. IEEE Transactions on Image Processing, 19(1):185–198, 2010.[14] J. Harel, C. Koch, and P. Perona. Graph-based visual saliency. In Annual Conference on Neural Information Processing Systems (NIPS), pages 545–552, 2007.[15] S. Hemami, F. Estrada, and S. Susstrunk. Frequency-tuned salient region detection. In Computer Vision and Pattern Recognition (CVPR), pages 1597–1604, 2009.[16] X. Hou and L. Zhang. Saliency detection: A spectral residual approach. In Computer Vision and Pattern Recognition (CVPR), pages 1–8, 2007.[17] L. Itti, C. Koch, and E. Niebur. A model of saliency-based visual attention for rapid scene analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(11):1254–1259, 1998.[18] H. Jiang, J. Wang, Z. Yuan, T. Liu, N. Zheng, and S. Li. Automatic salient object segmentation based on context and shape prior. In British Machine Vision Conference (BMVC), pages 1–12, 2011.[19] H. Jiang, J. Wang, Z. Yuan, Y. Wu, N. Zheng, and S. Li. Salient object detection: A discriminative regional feature integration approach. In Computer Vision and Pattern Recognition (CVPR), pages 2083–2090, June 2013.[20] T. Judd, K. Ehinger, F. Durand, and A. Torralba. Learning to predict where humans look. In International Conference on Computer Vision, 2009.[21] J. Kim, D. Han, Y.-W. Tai, and J. Kim. Salient region detection via high-dimensional color transform. In Computer Vision and Pattern Recognition (CVPR), pages 883–890, June 2014.[22] C. Koch and S. Ullman. Shifts in selective visual attention: Towards the underlying neural circuitry. In Matters of Intelligence, volume 188, pages 115–141. 1987.[23] I. Laurent. Automatic foveation for video compression using a neurobiological model of visual attention. IEEE Transactions on Image Processing, 13(10):1304–18, 2004.[24] X. Li, H. Lu, L. Zhang, X. Ruan, and M.-H. Yang. Saliency detection via dense and sparse reconstruction. In International Conference on Computer Vision (ICCV), pages 2976–2983, Dec 2013.[25] Y. Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille. The secrets of salient object segmentation. In Computer Vision and Pattern Recognition (CVPR), pages 280–287, 2014.[26] R. Liu, J. Cao, Z. Lin, and S. Shan. Adaptive partial differential equation learning for visual saliency detection. In Computer Vision and Pattern Recognition (CVPR), pages 3866–3873, June 2014.[27] T. Liu, Z. Yuan, J. Sun, J. Wang, N. Zheng, T. X., and S. H.Y. Learning to detect a salient object. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33, 2011.[28] Y. F. Ma and H. J. Zhang. Contrast-based image attention analysis by using fuzzy growing. In Eleventh ACM International Conference on Multimedia, pages 374–381, 2003.[29] L. Marchesotti, C. Cifarelli, and G. Csurka. A framework for visual saliency detection with applications to image thumbnailing. In International Conference on Computer Vision (ICCV), pages 2232–2239, 2009.[30] R. Margolin, A. Tal, and L. Zelnik-Manor. What makes a patch distinct? In Computer Vision and Pattern Recognition (CVPR), pages 1139–1146, June 2013.[31] J. Pan, Z. Su, M. Bian, and R. Liu. Saliency detection based on an edge-preserving filter. In 2013 20th IEEE International Conference on Image Processing (ICIP), pages 1757–1761, 2013.[32] F. Perazzi, P. Krähenbühl, Y. Pritch, and A. Hornung. Saliency filters: Contrast based filtering for salient region detection. In Computer Vision and Pattern Recognition (CVPR), pages 733–740, 2012.[33] Y. Qin, H. Lu, Y. Xu, and H. Wang. Saliency detection via cellular automata. In Computer Vision and Pattern Recognition (CVPR), pages 110–119, 2015.[34] C. Rother, V. Kolmogorov, and A. Blake. “GrabCut”–Interactive foreground extraction using iterated graph cuts. ACM TOG, 23(3):309–314, 2004.[35] H. P. S and J. Pujari. Content based image retrieval using color boosted salient points and shape features of an image. International Journal of Image Processing, pages 10–17, 2008.[36] A. Sharon, G. Meirav, B. Achi, and B. Ronen. Image segmentation by probabilistic bottom-up aggregation and cue integration. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(2):1–8, 2007.[37] X. Shen and Y. Wu. A unified approach to salient object detection via low rank matrix recovery. In Computer Vision and Pattern Recognition (CVPR), pages 853–860, June 2012.[38] B. W. Tatler. The central fixation bias in scene viewing: selecting an optimal viewing position independently of motor biases and image feature distributions. Journal of Vision, 7(14):4.1–17, 2007.[39] N. Tong, H. Lu, X. Ruan, and M.-H. Yang. Salient object detection via bootstrap learning. In Computer Vision and Pattern Recognition (CVPR), pages 1884–1892, 2015.[40] J. Van, de Weijer, T. Gevers, and A. D. Bagdanov. Boosting color saliency in image feature detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(1):150–156, 2006.[41] Y. Wei, F. Wen, W. Zhu, and J. Sun. Geodesic saliency using background priors. In European Conference on Computer Vision (ECCV), volume 7574 of Lecture Notes in Computer Science, pages 29–42. Springer Berlin Heidelberg, 2012.[42] Y. Xie and H. Lu. Visual saliency detection based on bayesian model. In IEEE International Conference on Image Processing (ICIP), pages 645–648, Sept 2011.[43] Y. Xie, H. Lu, and M.-H. Yang. Bayesian saliency via low and mid level cues. IEEE Transactions on Image Processing, 22(5):1689–1698, May 2013.[44] Q. Yan, L. Xu, J. Shi, and J. Jia. Hierarchical saliency detection. In Computer Vision and Pattern Recognition(CVPR), pages 1155–1162, June 2013.[45] C. Yang, L. Zhang, H. Lu, R. Xiang, and M. H. Yang. Saliency detection via graph-based manifold ranking. In Computer Vision and Pattern Recognition (CVPR), pages 3166–3173, 2013.[46] L. Zhang, Y. Shen, and H. Li. Vsi: A visual saliency-induced index for perceptual image quality assessment. IEEE Transactions on Image Processing, 23(10):4270–4281, Oct 2014.[47] L. Zhang, M. Tong, T. Marks, H. Shan, and G. Cottrell. SUN: A bayesian framework for saliency using natural statistics. Journal of Vision, 8(7):32:1–20, 2008.[48] W. Zhu, S. Liang, Y. Wei, and J. Sun. Saliency optimization from robust background detection. In Computer Vision and Pattern Recognition (CVPR), pages 2814–2821, June 2014.]]></content>
      <categories>
        <category>Papers</category>
      </categories>
      <tags>
        <tag>Saliency Map</tag>
        <tag>Image Processing</tag>
        <tag>Translation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[思维导图工具]]></title>
    <url>%2F2018%2F03%2F17%2Fmind-mapping-url%2F</url>
    <content type="text"><![CDATA[在这里推荐一个构建思维导图的工具。链接在这儿呢：幕布]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Tool</tag>
        <tag>Mind Mapping</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[狗年大吉！]]></title>
    <url>%2F2018%2F02%2F16%2Fhappy-dog-year-2018%2F</url>
    <content type="text"><![CDATA[新年快乐！祝大家狗年旺旺旺！！！]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k-近邻算法]]></title>
    <url>%2F2018%2F01%2F28%2FMLA-kNN%2F</url>
    <content type="text"><![CDATA[学习机器学习实战一书，记录一下。书上原代码都是以Python2版本编写，这边我是用Python3版本，所以会与书上代码有少许不同。 k-近邻算法简单地说，k-近邻算法采用测量不同特征值之间的距离方法来进行分类。 优缺点优点：精度高、对异常值不敏感、无数据输入假定缺点：计算复杂度高、空间复杂度高适用数据范围：数值型和标称型 工作原理存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。一般说来，我们只选择样本数据集中前k个最相似的数据，这就k-近邻算法中k的出处，通常k是不大于20的整数。最后选择k个最相似数据中出现次数最多的分类（投票规则），作为新数据的分类。 一般流程 收集数据：可使用任何方法 准备数据：数据格式结构化处理 分析数据：任意选择方法 训练算法：此步骤不适用于k-近邻算法 测试算法：计算错误率 使用算法：首先输入样本数据和结构化的输出结果，然后运行k-近邻算法判定输入数据分别属于哪个分类，最后应用对计算出的分类执行后续的处理 导入数据新建Python文件kNN.py，添加数据导入函数：1234def createDataSet(): group = np.array([[1.0, 1.1], [1.0, 1.0], [0, 0], [0, 0.1]]) labels = ['A', 'A', 'B', 'B'] return group, labels 当然该函数中的四组数据及其标签均是人为假定的，后期可通过读取txt、csv等文件来获取数据。 实现kNN算法这边采用的是欧氏距离公式来计算两个向量点之间的距离，公式为：$$d (x_a, x_b) = \sqrt{\sum_{p} \left( x^p_a - x^p_b \right)^2}$$算法的伪代码实现：对未知类别属性的数据集（测试集）中的每个点依次执行以下操作 计算已知类别数据集（训练集）中的点与当前点之间的距离； 按照距离递增次序排序； 选取与当前点距离最小的k个点； 确定前k个点所在类别的出现频率； 返回前k个点出现频率最高的类别作为当前点的预测分类。 Python代码实现：1234567891011121314# 分类器实现，参数：输入测试数据、训练集、训练标签、k值def classify(inX, dataSet, labels, k): dataSetSize = dataSet.shape[0] # 数据集大小，即训练样本数量 diffMat = np.tile(inX, (dataSetSize, 1)) - dataSet # 计算两矩阵元素级别上的差 sqDiffMat = diffMat ** 2 # 所有元素求平方 sqDistances = sqDiffMat.sum(axis=1) # 横轴上所有元素求和 [ 2.21 2. 0. 0.01] distances = sqDistances ** 0.5 # 开根号 [ 1.48660687 1.41421356 0. 0.1 ] sortedDistIndicies = distances.argsort() # 从小到大排序后返回其索引 [2 3 1 0] classCount = &#123;&#125; # 空字典，存储前k个数据的标签及其计数 for i in range(k): # 遍历距离最小的前k个，统计它们的标签数量 votelabel = labels[sortedDistIndicies[i]] # 依次获取该数据的标签 classCount[votelabel] = classCount.get(votelabel, 0) + 1 # 若字典中存在该标签，则在该值上直接加1；若不存在，则先初始化为0，再加1 sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0] # 返回降序排序后数量最多的标签的值 注： dataSet.shape返回的是一个tuple，里面有两个元素，简单来说就是数据集数组的行和列数，也就是数据集的样本数量和单个样本的特征数。 np.tile函数（import numpy as np）：输入按照函数参数右侧的数值或元祖来进行复制扩充自己。因此这边代码中np.tile(inX, (dataSetSize, 1))，实际上就是将inX整个数据在纵轴方向上复制扩充了dataSetSize倍，横轴方向上保持1倍，即不变。这样使得原数据inX可以与数据集dataSet中每个样本进行相减求差值。 sqDiffMat.sum(axis=1)：对sqDiffMat做求和操作，axis=1表示对列进行操作，但是是以横轴为方向（其实就是行上所有数求和）。所以这边做的是对测试样本与每个训练样本的差值的求和。若axis=0则表示对行进行操作，但是是以列为方向（列方向上所有数求和）。 sortedDistIndicies = distances.argsort()：将distances中的元素从小到大排列，提取其对应的index(索引)，然后输出到sortedDistIndicies。 sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)：classCount.items()返回的是一个列表，其中包含了由键值组成的元祖；operator.itemgetter(1)表示定义一个函数，获取第1个域；reverse=True表示逆序。所以整个sorted()函数做的是对classCount.items()这元祖列表做排序（逆序）操作，这排序的规则是按key来进行的，也就是根据列表中元祖的第1个域（即第2个值）来排序（其实就是根据各个标签的统计数量来降序排序）。 测试分类器错误率是常用的评估方法，主要用于评估分类器在某个数据集上的执行效果。分类器的错误率：分类器给出的错误结果的次数除以测试执行的总数。完美分类器的错误率为0，最差分类器的错误率为1.0。 简单地调用运行：123456import kNNif __name__ == '__main__': group, labels = kNN.createDataSet() label = kNN.classify([0, 0], group, labels, 3) print(label) 示例：使用k-近邻算法改进约会网站的配对效果准备数据数据文件：datingTestSet2.txt，每个样本（文件中每一行）中主要包含每一个对象的3种特征：每年获得的飞行常客里程数、玩视频游戏所耗时间百分比、每周消费的冰激凌公升数，最后一列是标签值：1、2、3，表示喜欢的程度。 在kNN.py文件中添加file2matrix函数，输入为文件名字符串，输出为训练样本矩阵和类标签向量。代码如下：1234567891011121314def file2matrix(filename): with open(filename, 'r') as file: # 打开文件 arrayLines = file.readlines() # 读取文件中所有行数据 numberOfLines = len(arrayLines) # 文件行数 returnMat = np.zeros((numberOfLines, 3)) # 创建返回的矩阵，初始化为0 classLabelVector = [] index = 0 for line in arrayLines: # 遍历行 line = line.strip() # 去除回车字符 listFromLine = line.split('\t') # 根据\t划分为列表 returnMat[index, :] = listFromLine[0:3] # 获取该行的前3个数据 classLabelVector.append(int(listFromLine[-1])) # 获取该样本数据的标签值 index += 1 return returnMat, classLabelVector # 返回样本数据数组和标签 调用函数，并打印数据：123dataArray, dataLabels = kNN.file2matrix("datingTestSet2.txt")print(dataArray)print(dataLabels[:10]) 数据打印显示为：12345678[[ 4.09200000e+04 8.32697600e+00 9.53952000e-01] [ 1.44880000e+04 7.15346900e+00 1.67390400e+00] [ 2.60520000e+04 1.44187100e+00 8.05124000e-01] ... [ 2.65750000e+04 1.06501020e+01 8.66627000e-01] [ 4.81110000e+04 9.13452800e+00 7.28045000e-01] [ 4.37570000e+04 7.88260100e+00 1.33244600e+00]][3, 2, 1, 1, 1, 1, 3, 3, 1, 3] 注： np.zeros((a, b))创建a行b列的数组，其值全为0 line.split(&#39;\t&#39;)对字符串按\t做划分操作，得到由多个子字符串组成的列表 归一化处理因为特征属性飞行常客里程数的数值较大，直接使用欧氏距离的话该属性值会严重影响计算结果。 因此，在处理这种不同取值范围的特征值时，通常采用的方法是将数值归一化，如将取值范围处理为0~1或者-1~1之间。下面公式将任意取值范围的特征值转化为0~1区间内的值：1newValue = (oldValue - min) / (max - min) 其中min和max分别是数据集中的最小特征值和最大特征值。 在kNN.py中添加新函数autoNorm()，该函数自动将数字特征值转化为0~1的区间：123456789def autoNorm(dataSet): minValues = dataSet.min(0) # 返回数据集中每列上的最小值 maxValues = dataSet.max(0) # 每列上的最大值 ranges = maxValues - minValues # 求差，得数据的范围 normDataSet = np.zeros(shape=np.shape(dataSet)) # 根据shape创建数组，全为0 m = dataSet.shape[0] # 样本数量 normDataSet = dataSet - np.tile(minValues, (m, 1)) # 公式 normDataSet = normDataSet / np.tile(ranges, (m, 1)) return normDataSet, ranges, minValues 测试算法在kNN.py中添加测试代码，作为完整程序来验证分类器：12345678910111213def dataClassTest(): testRatio = 0.20 # 定义数据集中为测试样本的比例 dataSet, dataLabels = file2matrix("datingTestSet2.txt") # 读取数据 normMat, ranges, minValues = autoNorm(dataSet) # 归一化处理 m = normMat.shape[0] # 样本数量 numTestVecs = int(m * testRatio) # 确定测试样本的数量 errorCount = 0.0 # 错误数量统计 for i in range(numTestVecs): classifierResult = classify(normMat[i, :], normMat[numTestVecs:m, :], dataLabels[numTestVecs:m], 3) # 传入测试数据、训练集、训练标签、k值 print("classify: %d, read answer: %d" % (classifierResult, dataLabels[i])) if classifierResult != dataLabels[i]: # 如果预测不正确，则统计加1 errorCount += 1 print("total error rate is: %f" % (errorCount / float(numTestVecs))) # 打印错误率 执行结果显示：12345678classify: 3, read answer: 3classify: 2, read answer: 2......classify: 2, read answer: 2classify: 3, read answer: 3classify: 2, read answer: 2total error rate is: 0.080000 构建完整系统1234567891011def classifyPerson(): resultList = ['not at all', 'in small doses', 'in large doses'] # 定义三种喜欢程度，对应数据集中标签 1,2,3 ffMiles = float(input("frequent flier miles earned per year?")) # 输入每年飞行里程数 percentTats = float(input("percentage of time spent playing video games?")) # 输入玩游戏所耗时间百分比 iceCream = float(input("liters of ice cream consumed per week?")) # 输入每周消费冰激凌公升数 dataArray, dataLabels = file2matrix("datingTestSet2.txt") # 从txt中获取训练数据 normMat, ranges, minVals = autoNorm(dataArray) # 归一化处理 inArray = np.array([ffMiles, percentTats, iceCream]) # 对测试数据处理，整合成数组 normInArray = (inArray - minVals) / ranges # 对数据做归一化处理 classifyResult = classify(normInArray, normMat, dataLabels, 3) # 分类，k=3 print("you will probably like this person: ", resultList[classifyResult - 1]) 只要输入对应特征的值，就可以获得系统的结果判定，运行结果显示：1234percentage of time spent playing video games?12frequent flier miles earned per year?30000liters of ice cream consumed per year?0.5you will probably like this person: in large doses 示例：手写识别系统准备数据数字图像均已存储为txt形式，里面是由32行32列的0和1数字组成 首先把32x32的二进制图像转换为1x1024的向量，这样就可以用之前的分类器来处理这些数字图像信息了。123456789# 将图像转为向量def img2vector(filename): returnVector = np.zeros((1, 1024)) # 初始化0数组，1行1024列 with open(filename, 'r') as file: # 读取文件 for i in range(32): # 遍历行 lineStr = file.readline() # 读取行 for j in range(32): # 遍历列 returnVector[0, 32 * i + j] = int(lineStr[j])# 将该行上第j个数据存进数组第i行第j列中 return returnVector # 返回数组 测试算法上边我们已将数据处理为分类器可以识别的格式，现在我们将这些数据输入到分类器中，检测分类器的执行效果。 kNN.py中需添加from os import listdir，用于列出给定目录的文件名。 12345678910111213141516171819202122232425def handwritingClassTest(): hwLabels = [] # 列表，存放训练数据集标签 trainingFileList = listdir("digits/trainingDigits") # 列出给定目录中的文件名 m = len(trainingFileList) # 训练样本数 trainingMat = np.zeros((m, 1024)) # 初始化全0矩阵 m行1024列 for i in range(m): # 遍历训练数据 fileNameStr = trainingFileList[i] # 获取文件名全称，如 3_107.txt fileStr = fileNameStr.split('.')[0] # 根据 . 划分，获取文件名 如 3_107 classNum = int(fileStr.split('_')[0]) # 根据 _ 划分，获取该文件表示的真实数字 如 3 hwLabels.append(classNum) # 将该数字标签放入训练集标签列表中 trainingMat[i, :] = img2vector('digits/trainingDigits/%s' % fileNameStr) # 调用函数，将第i个文件内的内容转化为数组，并存储 testFileList = listdir("digits/testDigits") # 列出测试集目录中的文件名 errorCount = 0.0 # 错误统计 mTest = len(testFileList) # 测试集大小 for i in range(mTest): # 遍历测试集 fileNameStr = testFileList[i] fileStr = fileNameStr.split('.')[0] classNum = int(fileStr.split('_')[0]) vectorUnderTest = img2vector("digits/testDigits/%s" % fileNameStr) classifyResult = classify(vectorUnderTest, trainingMat, hwLabels, 3) # 调用函数，预测数字 print("the classifier: %d, the real value: %d" % (classifyResult, classNum)) if classifyResult != classNum: errorCount += 1.0 print("total number of errors: %d" % errorCount) print("total error rate: %f" % (errorCount / float(mTest))) # 错误率 调用运行：1kNN.handwritingClassTest() 结果显示：12345678the classifier: 0, the real value: 0the classifier: 0, the real value: 0...the classifier: 9, the real value: 9the classifier: 9, the real value: 9the classifier: 9, the real value: 9total number of errors: 10total error rate: 0.010571 小结本篇主要就是学习k-近邻算法，以及该算法的两个简单实际应用。k-近邻算法必须保存全部数据集，如果训练数据集很大，必须使用大量的存储空间。由于必须对数据集中的每个数据计算距离值，实际使用时非常耗时。 代码链接：https://github.com/asdfv1929/MachineLearningInAction_Python3/tree/master/Chapter02_kNN]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>kNN</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo NexT主题内给每篇文章后添加结束标语]]></title>
    <url>%2F2018%2F01%2F28%2Fadd-the-end%2F</url>
    <content type="text"><![CDATA[给文章后面添加结束标语 新建文件在\themes\next\layout\_macro中新建passage-end-tag.swig文件，添加代码至该文件中：12345&lt;div&gt; &#123;% if not is_index %&#125; &lt;div style=&quot;text-align:center;color: #ccc;font-size:14px;&quot;&gt;-------------本文结束&lt;i class=&quot;fa fa-paw&quot;&gt;&lt;/i&gt;感谢您的阅读-------------&lt;/div&gt; &#123;% endif %&#125;&lt;/div&gt; 修改post.swig打开\themes\next\layout\_macro\post.swig文件，在post-body后，post-footer前，添加下面内容：12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include &apos;passage-end-tag.swig&apos; %&#125; &#123;% endif %&#125;&lt;/div&gt; 修改_config打开主题配置文件（_config.yml),在末尾添加：123# 文章末尾添加“本文结束”标记passage_end_tag: enabled: true 至此，就完成了关于添加文章结束标语的功能，具体的效果，此刻，想必你也看到了，就在下边。]]></content>
      <categories>
        <category>GitBlog</category>
      </categories>
      <tags>
        <tag>NexT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown中MathJax转义问题]]></title>
    <url>%2F2018%2F01%2F28%2Fmarkdown-mathjax%2F</url>
    <content type="text"><![CDATA[Markdown本身的特殊符号与Latex中的符号会出现冲突，导致网页中不能正确显示数学公式。 问题例如：_的转义，在Markdown中，_是斜体，但是在Latex中，却有下标的意思，就会出现问题；\\的换行，在Markdown中，\\会被转义为\,这样也会影响影响Mathjax对公式中的\\进行渲染 原因Hexo默认使用marked.js去解析我们写的Markdown，比如一些符号，_代表斜体，会被处理为&lt;em&gt;标签，比如x_i在开始被渲染的时候，处理为x&lt;em&gt;i&lt;/em&gt;，这个时候MathJax就无法渲染成下标了。 解决找到nodes_modules/lib/marked/lib/marked.js文件，将文件中代码：1escape: /^\\([\\`*&#123;&#125;\[\]()# +\-.!_&gt;])/, 改为：1escape: /^\\([`*&#123;&#125;\[\]()# +\-.!_&gt;])/, 找到：1em: /^\b_((?:[^_]|__)+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 改为：1em:/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 做了上边修改后，网页中的数学公式正常显示了。]]></content>
      <categories>
        <category>GitBlog</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
        <tag>MathJax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo NexT主题添加点击爱心效果]]></title>
    <url>%2F2018%2F01%2F26%2Fclick-love%2F</url>
    <content type="text"><![CDATA[给NexT主题内添加页面点击出现爱心的效果 创建js文件在/themes/next/source/js/src下新建文件clicklove.js，接着把该链接下的代码拷贝粘贴到clicklove.js文件中。代码如下：1!function(e,t,a)&#123;function n()&#123;c(&quot;.heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: &apos;&apos;;width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;&quot;),o(),r()&#125;function r()&#123;for(var e=0;e&lt;d.length;e++)d[e].alpha&lt;=0?(t.body.removeChild(d[e].el),d.splice(e,1)):(d[e].y--,d[e].scale+=.004,d[e].alpha-=.013,d[e].el.style.cssText=&quot;left:&quot;+d[e].x+&quot;px;top:&quot;+d[e].y+&quot;px;opacity:&quot;+d[e].alpha+&quot;;transform:scale(&quot;+d[e].scale+&quot;,&quot;+d[e].scale+&quot;) rotate(45deg);background:&quot;+d[e].color+&quot;;z-index:99999&quot;);requestAnimationFrame(r)&#125;function o()&#123;var t=&quot;function&quot;==typeof e.onclick&amp;&amp;e.onclick;e.onclick=function(e)&#123;t&amp;&amp;t(),i(e)&#125;&#125;function i(e)&#123;var a=t.createElement(&quot;div&quot;);a.className=&quot;heart&quot;,d.push(&#123;el:a,x:e.clientX-5,y:e.clientY-5,scale:1,alpha:1,color:s()&#125;),t.body.appendChild(a)&#125;function c(e)&#123;var a=t.createElement(&quot;style&quot;);a.type=&quot;text/css&quot;;try&#123;a.appendChild(t.createTextNode(e))&#125;catch(t)&#123;a.styleSheet.cssText=e&#125;t.getElementsByTagName(&quot;head&quot;)[0].appendChild(a)&#125;function s()&#123;return&quot;rgb(&quot;+~~(255*Math.random())+&quot;,&quot;+~~(255*Math.random())+&quot;,&quot;+~~(255*Math.random())+&quot;)&quot;&#125;var d=[];e.requestAnimationFrame=function()&#123;return e.requestAnimationFrame||e.webkitRequestAnimationFrame||e.mozRequestAnimationFrame||e.oRequestAnimationFrame||e.msRequestAnimationFrame||function(e)&#123;setTimeout(e,1e3/60)&#125;&#125;(),n()&#125;(window,document); 修改_layout.swig在\themes\next\layout\_layout.swig文件末尾添加：12&lt;!-- 页面点击小红心 --&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/clicklove.js&quot;&gt;&lt;/script&gt;]]></content>
      <categories>
        <category>GitBlog</category>
      </categories>
      <tags>
        <tag>NexT</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo NexT主题中添加网页标题崩溃欺骗搞怪特效]]></title>
    <url>%2F2018%2F01%2F25%2Fcrash-cheat%2F</url>
    <content type="text"><![CDATA[给网页title添加一些搞怪特效 crash_cheat.js在next\source\js\src文件夹下创建crash_cheat.js，添加代码：1234567891011121314151617&lt;!--崩溃欺骗--&gt; var OriginTitle = document.title; var titleTime; document.addEventListener(&apos;visibilitychange&apos;, function () &#123; if (document.hidden) &#123; $(&apos;[rel=&quot;icon&quot;]&apos;).attr(&apos;href&apos;, &quot;/img/TEP.ico&quot;); document.title = &apos;╭(°A°`)╮ 页面崩溃啦 ~&apos;; clearTimeout(titleTime); &#125; else &#123; $(&apos;[rel=&quot;icon&quot;]&apos;).attr(&apos;href&apos;, &quot;/favicon.ico&quot;); document.title = &apos;(ฅ&gt;ω&lt;*ฅ) 噫又好了~&apos; + OriginTitle; titleTime = setTimeout(function () &#123; document.title = OriginTitle; &#125;, 2000); &#125; &#125;); 引用在next\layout\_layout.swig文件中，添加引用（注：在swig末尾添加）：12&lt;!--崩溃欺骗--&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/crash_cheat.js&quot;&gt;&lt;/script&gt;]]></content>
      <categories>
        <category>GitBlog</category>
      </categories>
      <tags>
        <tag>NexT</tag>
        <tag>Git</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo NexT主题内接入网页在线联系功能]]></title>
    <url>%2F2018%2F01%2F21%2Fdaovoice%2F</url>
    <content type="text"><![CDATA[之前有访问过一些大佬的个人博客，里面有个在线联系功能，看着不错，所以也试着在自己的站点上接入了此功能。 注册首先在DaoVoice注册个账号，点击-&gt;邀请码是2e5d695d。 完成后，会得到一个app_id，后面会用到： 修改head.swig修改/themes/next/layout/_partials/head.swig文件，添加内容如下：123456789&#123;% if theme.daovoice %&#125; &lt;script&gt; (function(i,s,o,g,r,a,m)&#123;i[&quot;DaoVoiceObject&quot;]=r;i[r]=i[r]||function()&#123;(i[r].q=i[r].q||[]).push(arguments)&#125;,i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset=&quot;utf-8&quot;;m.parentNode.insertBefore(a,m)&#125;)(window,document,&quot;script&quot;,(&apos;https:&apos; == document.location.protocol ? &apos;https:&apos; : &apos;http:&apos;) + &quot;//widget.daovoice.io/widget/0f81ff2f.js&quot;,&quot;daovoice&quot;) daovoice(&apos;init&apos;, &#123; app_id: &quot;&#123;&#123;theme.daovoice_app_id&#125;&#125;&quot; &#125;); daovoice(&apos;update&apos;); &lt;/script&gt;&#123;% endif %&#125; 位置贴图： 主题配置文件在_config.yml文件中添加内容：123# Online contactdaovoice: truedaovoice_app_id: # 这里填你刚才获得的 app_id 聊天窗口配置附上我的聊天窗口的颜色、位置等设置信息： 至此，网页的在线联系功能已经完成，重新hexo g，hexo d上传GitHub后，页面上就能看到效果了。 就比如说你现在往右下角看看(～￣▽￣)～ ，欢迎撩我（滑稽）。]]></content>
      <categories>
        <category>GitBlog</category>
      </categories>
      <tags>
        <tag>NexT</tag>
        <tag>Hexo</tag>
        <tag>DaoVoice</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Yesky网镁铝图片下载]]></title>
    <url>%2F2018%2F01%2F20%2Fyesky-image-spider%2F</url>
    <content type="text"><![CDATA[看到这么多漂亮的妹纸，内心是久久不能平静啊（滑稽脸￣ω￣=）。这不，只能靠记录下怎么获取妹子们图片的流程来安稳心神了ヽ(￣▽￣)ﾉ。 Model创建了一个类，初始化：1234def __init__(self, url, title): self.url = url self.title = title self.img_urls_list = [] # 列表，存储图片的链接 读取网页源码：123456789# 获取网页源码，带容错def get_html(self, url, k=3): count = 0 while count &lt; k: html = requests.get(url) if html: return html.text count += 1 return None 这是将BeautifulSoup解析抽成一个方法，便于后面调用：1234567def get_html_soup(self, url): html = self.get_html(url) if not html: print("It does not get the html.") return None html_soup = BeautifulSoup(html, 'html.parser') return html_soup 获取某一页面上的镁铝图片链接：1234567891011# 返回该页面上的图片链接def get_img(self, url): html_soup = self.get_html_soup(url) if not html_soup: return None select_list = html_soup.select(".l_effect_img_mid a img") if not select_list: print("select mistake.") return None item = select_list[0] return item['src'] 保存所有链接到列表：12345678910# 保存所有图片的链接到列表（嵌套列表）def get_img_urls(self): html_soup = self.get_html_soup(self.url) img_set = [] select_list = html_soup.select(".overview ul li a") if select_list: for item in select_list: img_url = self.get_img(item['href']) img_set.append(img_url) self.img_urls_list.append(img_set) 最后便是保存图片，其中涉及到对标题需做一定的修改，确保文件夹创建成功：12345678910111213141516171819202122# 获取图片的名称，如 77VQ96UV4D9F.jpgdef get_img_name(self, url): return url.split('/')[-1]# 消除一些影响创建文件夹的特殊字符def del_special_symbol(self, string): return string.replace(':', '').replace('"', '').replace('?', '').strip()# 保存图片至本地def download_img(self, urls_list, file_path): for url in urls_list: # 遍历一组照片链接列表 urllib.request.urlretrieve(url, file_path + '\\' + self.get_img_name(url)) # 下载图片# 创建文件夹，并保存图片def save_to_dir(self): for item in self.img_urls_list: # 遍历各组图片链接列表 print(self.title, ' downloading...') file_path = 'yeskyImgs\%s' % self.del_special_symbol(self.title) # 路径 if not os.path.exists(file_path): os.makedirs(file_path) # 创建该组图片的文件夹 th = threading.Thread(target=self.download_img(item, file_path)) # 下载该组图片 th.start() All Images1234567891011121314151617181920def get_html(url, k=3): count = 0 while count &lt; k: html = requests.get(url) if html: return html.text count += 1 return Nonedef get_info(html): html_soup = BeautifulSoup(html, 'html.parser') select_list = html_soup.select(".lb_box dl dd a") if select_list: for item in select_list: url = item['href'] # 跳转链接 title = item.string # 该组图片的标题 model = AModel(url, title) model.get_img_urls() # 获取该组图片的所有链接 model.save_to_dir() # 保存图片到文件夹下 Run12345678if __name__ == '__main__': url = 'http://pic.yesky.com/c/6_20491_%s.shtml' start = time.time() for page in range(1, 6): html = AllPics.get_html(url % page) AllPics.get_info(html) end = time.time() print('%.2f seconds' % (end - start)) 图片下载到本地后的结果： 附上程序：https://github.com/asdfv1929/YeskyImagesSpider]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>BeautifulSoup</tag>
        <tag>Spider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo NexT主题中集成gitalk评论系统]]></title>
    <url>%2F2018%2F01%2F20%2Fgitalk%2F</url>
    <content type="text"><![CDATA[记录在NexT主题中添加gitalk评论系统的步骤。gitalk：一个基于 Github Issue 和 Preact 开发的评论插件详情Demo可见：https://gitalk.github.io/ Register Application在GitHub上注册新应用，链接：https://github.com/settings/applications/new参数说明：Application name： # 应用名称，随意Homepage URL： # 网站URL，如https://asdfv1929.github.ioApplication description # 描述，随意Authorization callback URL：# 网站URL，https://asdfv1929.github.io 点击注册后，页面跳转如下，其中Client ID和Client Secret在后面的配置中需要用到，到时复制粘贴即可： gitalk.swig新建/layout/_third-party/comments/gitalk.swig文件，并添加内容：1234567891011121314151617&#123;% if page.comments &amp;&amp; theme.gitalk.enable %&#125; &lt;link rel=&quot;stylesheet&quot; href=&quot;https://unpkg.com/gitalk/dist/gitalk.css&quot;&gt; &lt;script src=&quot;https://unpkg.com/gitalk/dist/gitalk.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; var gitalk = new Gitalk(&#123; clientID: &apos;&#123;&#123; theme.gitalk.ClientID &#125;&#125;&apos;, clientSecret: &apos;&#123;&#123; theme.gitalk.ClientSecret &#125;&#125;&apos;, repo: &apos;&#123;&#123; theme.gitalk.repo &#125;&#125;&apos;, owner: &apos;&#123;&#123; theme.gitalk.githubID &#125;&#125;&apos;, admin: [&apos;&#123;&#123; theme.gitalk.adminUser &#125;&#125;&apos;], id: location.pathname, distractionFreeMode: &apos;&#123;&#123; theme.gitalk.distractionFreeMode &#125;&#125;&apos; &#125;) gitalk.render(&apos;gitalk-container&apos;) &lt;/script&gt;&#123;% endif %&#125; comments.swig修改/layout/_partials/comments.swig，添加内容如下，与前面的elseif同一级别上：12&#123;% elseif theme.gitalk.enable %&#125; &lt;div id=&quot;gitalk-container&quot;&gt;&lt;/div&gt; index.swig修改layout/_third-party/comments/index.swig，在最后一行添加内容：1&#123;% include &apos;gitalk.swig&apos; %&#125; gitalk.styl新建/source/css/_common/components/third-party/gitalk.styl文件，添加内容：1234.gt-header a, .gt-comments a, .gt-popup a border-bottom: none;.gt-container .gt-popup .gt-action.is--active:before top: 0.7em; third-party.styl修改/source/css/_common/components/third-party/third-party.styl，在最后一行上添加内容，引入样式：1@import &quot;gitalk&quot;; _config.yml在主题配置文件next/_config.yml中添加如下内容：12345678gitalk: enable: true githubID: github帐号 # 例：asdfv1929 repo: 仓库名称 # 例：asdfv1929.github.io ClientID: Client ID ClientSecret: Client Secret adminUser: github帐号 #指定可初始化评论账户 distractionFreeMode: true 以上就是NexT中添加gitalk评论的配置，博客上传到GitHub上后，打开页面进入某一博客内容下，就可看到评论处。]]></content>
      <categories>
        <category>GitBlog</category>
      </categories>
      <tags>
        <tag>NexT</tag>
        <tag>Hexo</tag>
        <tag>Gitalk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cs231n学习记录 Part1 Image Classification]]></title>
    <url>%2F2018%2F01%2F14%2Fcs231n-part-1-image-classification%2F</url>
    <content type="text"><![CDATA[此为学习斯坦福cs231n课程所做笔记。 图像分类动机本节主要讲的是图像分类问题，它是一个从固定的类别集合里分配一个标签（label）给输入图像的任务。这是计算机视觉的核心问题，虽然看似简单，但是有很多的实际应用。其他许多看似不同的计算机视觉任务（比如说：目标检测，图像分割）都可以归类为图像分类问题。 举例举个栗子，上图中，图像分类模型接收了一个图像，然后分配给它4个标签（cat、dog、hat、mug）。如图所示，对于计算机来说，图像被视为是一个大型的3维数组。在这里，猫图有248像素宽，400像素高，且有三个颜色通道：Red、Green、Blue（即RGB）。因此，该图像是由 248 x 400 x 3 个数字组成，也就是总共297600个数字。其中每个数字都是一个在0（black）到255（white）范围内的整数值。图像分类的任务就是将这组几十万的数字转换成单个标签label，比如说“cat”。 挑战因为对于人类来说识别一个视觉目标（比如说，猫）是非常容易的，但是从计算机视觉算法的角度上来考虑，其中会涉及到许多挑战。下面列出部分挑战： Viewpoint variation（视角变化）：一个目标实体对于照相机来说可以多种方式进行定向。 Scale variation（规模变化）：视觉目标通常会存在多种大小变化（真实世界中的大小，不仅仅是图像中的大小程度） Deformation（形变）：许多目标实体不是那种外形固定的物体，可能会以极端方式发生变形。 Occlusion（遮挡）：目标实体可能会被遮挡，有时候目标只有一小部分（只有少数像素）可见。 Illumination conditions（光照条件）：光照条件在像素级上影响较大。 Background clutter（背景混乱）：目标对象可能混入其环境之中，使得他们难以被识别出来。 Intra-class variation（类内变化）：目标对象所属的类别可能有多种外观，比如说椅子。这些目标是不同的类型，且每个类型都有自己的外观。 一个好的图像分类模型必须是对这些变化的交叉组合保持不变性，同时对类间变化保持敏感性。 数据驱动方法提供给计算机诸多类，其中每一类都包含许多样本，之后研发出一个学习算法去训练这些样本，学习每一类的视觉外观。这种方法被称为数据驱动方法，因为它依赖于收集标注过的图像训练数据集。如下图所示： 图像分类流程图像分类的任务是输入代表图像的像素数组，然后输出一个分配给它的标签值。 完整的图像分类流程大致如下： 输入：输入N个图像，其中每个图像都被标注为K个不同类别中的一个，称之为训练集 学习：任务是学习训练集，学习每一类外观是什么样子的。可称这个步骤为训练一个分类器，或学习一个模型 评估：最后，通过预测新的图像集合中的图像标签来评估这个分类器的性能 最近邻分类器最近邻分类器，与卷积神经网络无关，但却非常的实用，它给出了图像分类问题的一个基本解决方法的理念。 CIFAR-10数据集图像分类数据集CIFAR-10包含了60000张32x32像素的小图片，每个图片都被标注为10个类别中的一个。这6万张图片被划分为含有50000张图片的训练集和含有10000张图片的测试集。 下图中可以看到数据集中每类各10张的随机图像样本：上图左侧是CIFAR-10数据集的样例图像；右侧中第一列是测试图像，之后的列中是根据像素级距离来判断的与第一列中测试图像距离最近的10张训练图像。 最近邻分类器接收一个测试图像，将之与训练集中的每个图像进行比较，之后输出距离最近的训练图像的标签值，即为分类器的预测值。在上图的右侧中我们可以看到根据测试图像生成的距离最近的10张训练图像，从右侧的那些距离最近的训练图像中我们可以发现，测试图像中只有少数几个检索到同一类的目标图像，其余的都不是该类图像。比如说，在第8行中，与测试图像（马）距离最近的训练图像中却是一辆车，这可能是因为两幅图中大范围的黑色背景所导致。因此，该马类图像在这种情况下被错误地标注为是一辆车。 L1 Distance接下来介绍下如何对两幅图像进行比较，也就是两个32x32x3的块之间的比较。一个最简单的方法是逐个比较图像的像素，并对其中所有的差值求和。换句话说，给定两张图像，分别表示为$I_1$、$I_2$，比较它们的一个合理方法便是L1距离：$$d_1 (I_1, I_2) = \sum_{p} \left| I^p_1 - I^p_2 \right|$$其中求和操作是面向所有像素的。 过程可视化：上图是一个在像素级上使用L1距离对两张图像进行比较的简单例子（例子中只是列出一个通道）。两幅图像各元素对应值相减，然后将所有的差值相加，得到单个数值。如果两张图像相同，则这单个数值将会是0；如果两张图像差异较大，则该值相对较大。 代码实现首先载入数据为4个数组：训练数据/标签、测试数据/标签。Xtr（size: 50000x32x32x3）包含了训练集中的所有图像，对应的一维数组Ytr（length: 50000）包含了训练标签（0~9）：1234Xtr, Ytr, Xte, Yte = load_CIFAR10('data/cifar10/') # 魔法函数，获取数据# 扁平化处理Xtr_rows = Xtr.reshape(Xtr.shape[0], 32 * 32 * 3) # Xtr_rows 50000 x 3072Xte_rows = Xte.reshape(Xte.shape[0], 32 * 32 * 3) # Xte_rows 10000 x 3072 之后便是训练与评估：12345nn = NearestNeighbor() # 创建一个最近邻分类器的实例nn.train(Xtr_rows, Ytr) # 在训练集和标签上训练分类器Yte_predict = nn.predict(Xte_rows) # 在测试图像上预测标签值# 打印输出分类准确度print 'accuracy: %f' % ( np.mean(Yte_predict == Yte) ) 注：1）一般都是用准确度来作为评估标准，预测正确的数量占比。2）所有分类器都有一个公共的接口：train(X, y)方法，表示从数据和标签中学习模型。3）predict(X)方法，接收新的数据，并预测其标签值。 下面是一个使用L1距离的简单最近邻分类器的代码实现：123456789101112131415161718192021222324252627import numpy as npclass NearestNeighbor(object): def __init__(self): pass def train(self, X, y): """ X is N x D where each row is an example. Y is 1-dimension of size N """ # the nearest neighbor classifier simply remembers all the training data self.Xtr = X self.ytr = y def predict(self, X): """ X is N x D where each row is an example we wish to predict label for """ num_test = X.shape[0] # 测试样本数量 # lets make sure that the output type matches the input type Ypred = np.zeros(num_test, dtype = self.ytr.dtype) # 初始化预测值全为0 # loop over all test rows for i in xrange(num_test): # 遍历所有测试图像 # find the nearest training image to the i'th test image # using the L1 distance (sum of absolute value differences) distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1) min_index = np.argmin(distances) # get the index with smallest distance Ypred[i] = self.ytr[min_index] # predict the label of the nearest example return Ypred 距离度量的选择向量之间有多种方式计算它们的距离。另一种常见的方法便是L2距离，其具有计算两个向量之间欧氏距离的几何解释。距离公式如下：$$d_2 (I_1, I_2) = \sqrt{\sum_{p} \left( I^p_1 - I^p_2 \right)^2}$$也是先计算像素级别上的差值，然后对其值求平方，之后将所有的值相加，最后对值开根号。代码方面，只要对之前的代码做些许修改即可，将计算距离的那行代码替换为如下一行：1distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1)) 上面代码中使用了np.sqrt，但在实际最近邻应用中，我们省去了开根号这一操作，因为开根是一个单调函数。也就是说，它控制了距离的绝对大小的量级，但仍保留其排序，所以有或没有开根，最近邻的比较没有影响（其实就是开根操作没有影响单调性）。 k近邻分类器k近邻分类器：找出与测试图像最相近的k个训练图像，然后根据投票规则（哪一类数量多，就选哪类），预测出测试图像的标签。特例，当k=1时，就变成最近邻分类器。直观上，较大的k值具有平滑效果，会使得分类器对异常值更具抵抗性，分类的准确度会更高。上图中，有色区域代表的是根据L2距离分类器做出的决策边界。在最近邻中，由于可能不正确的预测而导致出现一些与周围颜色不一致的“岛屿”，5-近邻中却将这些异常值做了相应处理，使得这些颜色边界更加平滑，这可能对测试数据会产生更好的泛化。右图中的灰色区域是由投票规则所导致的（2票红色，2票蓝色，最后一票绿色）。 超参数调优、验证集超参数k近邻分类器中的k值该如何选择，才会让分类器效果最优。之前距离度量函数的选择，包括L1、L2范数，这些选择都被称为“超参数”。 一个最简单的想法是，我们尝试许多不同的值，然后看它们中哪一个会让分类器效果更好。但是这不能通过使用测试集来调整超参数。1Evaluate on the test set only a single time, at the very end. 验证集所以换一种方式去调整超参数：将训练集划分成两个集合，一个是稍微变小的训练集，另一个则是验证集。用CIFAR-10来做个栗子，我们可以使用训练集中的49000张图像来训练，剩下的1000张图像用来验证。验证集就好比是一个伪测试集，用来对超参数进行调优的。 对CIFAR-10进行验证集划分：123456789101112131415161718192021# assume we have Xtr_rows, Ytr, Xte_rows, Yte as before# recall Xtr_rows is 50,000 x 3072 matrixXval_rows = Xtr_rows[:1000, :] # take first 1000 for validationYval = Ytr[:1000]Xtr_rows = Xtr_rows[1000:, :] # keep last 49,000 for trainYtr = Ytr[1000:]# find hyperparameters that work best on the validation setvalidation_accuracies = []for k in [1, 3, 5, 10, 20, 50, 100]: # use a particular value of k and evaluation on validation data nn = NearestNeighbor() nn.train(Xtr_rows, Ytr) # here we assume a modified NearestNeighbor class that can take a k as input Yval_predict = nn.predict(Xval_rows, k = k) acc = np.mean(Yval_predict == Yval) print 'accuracy: %f' % (acc,) # keep track of what works on the validation set validation_accuracies.append((k, acc)) 交叉验证有时候训练集的规模会比较小，这时候我们可以使用一种叫交叉验证的常用方法。比如说，5折交叉验证中，我们将训练集划分为5个相等数量的子集，使用其中的4个组成训练集，剩下的一个则为验证集。之后我们迭代选择其中一个为验证集，评估每一次的性能，最后对这5次评估取平均值。上图是k近邻分类器使用5折交叉验证的样例。对于k的每一个值，都是训练4个子集组成的训练集，在第5个子集合上评估。因此，每个k值都得到了5个准确度值。图中的折线是由每个k的平均准确度相连得到。图中表明，在交叉验证的情况下，当k=7时，准确度相对最高（峰值）。 实际应用中，由于交叉验证在计算上消耗太大，人们一般都偏向于选择将训练数据中的50%-90%作为训练集，其余的作为验证集。但是若是你的训练样本非常少，建议使用交叉验证。常见的交叉验证的折数可以是3折、5折或10折。 最近邻分类器的优缺点优点： 易于理解和实现 不需要时间去训练，训练过程中所需要做的只是存储和对训练数据进行索引 缺点： 测试时计算消耗太大，因为需要去将测试图像与每一个训练图像进行比较。测试时间的效率比训练时更重要 计算复杂度较高 虽说在某些情况下，最近邻的确是一个很好的选择，但是它却很少被应用于实际图像分类环境中去。其中的一个问题便是，图像是一个高维对象（通常包含许多像素），在这高维空间中距离很难被直观地表现出来。 下图便阐明了由之前基于L2距离的相似与感官上的相似有很大不同这方面。最左侧是原图，右侧三幅图分别对原图做了一些变化。然而这三幅图在L2像素距离上与原图有很大的差值。因此，像素级上的距离与感官或语义上的相似是完全不相符的。 总结 图像分类，训练集，测试集 最近邻分类器，超参数 验证集 交叉验证 评估 L1、L2距离]]></content>
      <categories>
        <category>cs231n</category>
      </categories>
      <tags>
        <tag>kNN</tag>
        <tag>Image Classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git Bash上传项目命令]]></title>
    <url>%2F2018%2F01%2F13%2Fgit-note%2F</url>
    <content type="text"><![CDATA[记录下平时将项目demo上传至GitHub Repository上的命令。 首先是打开需要上传的项目，切换至该文件夹下，右击Git Bash Here，在Git命令行界面中依次输入以下指令： 将当前文件夹初始化为GitHub项目文件夹：1git init 添加项目中所有文件：1git add . 设置项目提交后文件上显示的备注信息：1git commit -m &quot;first commit&quot; 添加远程项目仓库：1git remote add origin https://github.com/asdfv1929/test.git 最后push项目到仓库master分支：1git push -u origin master]]></content>
      <categories>
        <category>GitBlog</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>GitHub</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[全书网小说内容爬取]]></title>
    <url>%2F2018%2F01%2F13%2Fquanshu-novel-net-spider%2F</url>
    <content type="text"><![CDATA[本篇主要记录的是关于小说网的简单爬取，并将内容保存至txt文件中。 前期准备准备工作的话，一方面Python语言，另一方面就是几个Python库的使用：requests、bs4等。关于这些库的具体用法，我这边就不描述了，详细的内容可见官方文档或某些博客：requests：http://docs.python-requests.org/zh_CN/latest/user/quickstart.htmlbs4：https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html 小说网链接：http://www.quanshuwang.com/list/1_1.html 小说模型我是定义了一个小说类模型，用来存储或操作一篇小说的相关信息。 首先是，通过小说的一些数据信息对小说进行初始化：12345678def __init__(self, title=None, author=None, cover=None, desc=None, novel_url=None, chapters_url=None): self.NovelTitle = title # 小说名 self.Author = author # 小说作者 self.CoverImgUrl = cover # 封面图片链接 self.Desc = desc # 小说内容的简短介绍 self.NovelUrl = novel_url # 小说跳转链接 self.ChaptersUrl = chapters_url # 小说目录链接 self.Chapters = [] # 小说所有章节(chapterTitle, chapterUrl)列表 之后，我简单定义了一个获取网页源码的方法：12345# 读取传入参数url的网页源码 def readHtml(self, url): html = requests.get(url) # 请求url html.encoding = 'gbk' return html.text 接着便是获取该小说所有章节的标题及章节链接：1234567# 获取该小说所有章节的(title, url)组成的列表Chapters def getChaptersUrl(self): html = self.readHtml(self.ChaptersUrl) soup = BeautifulSoup(html, 'html.parser') for chapter in soup.select(".chapterNum ul li a"): self.Chapters.append((chapter.string, chapter['href'])) # 章节名及链接加入列表 return self.Chapters 最后便是通过章节链接，去访问该章节对应的小说内容，当然，里面还包括了将内容存入本地txt：1234567891011121314151617# 获取该小说某一章节的小说文本内容，并写入文件 chapter: (chapter_title, chapter_url) def getOneChapterContent(self, chapter): k = 0 obj = None while k &lt; 3: # 容错 html = self.readHtml(chapter[1]) soup = BeautifulSoup(html, 'html.parser') obj = soup.find(attrs="mainContenr") if obj: break k += 1 if not obj: print('None') return content = obj.text content_drop = content.replace("style5();", "").replace("style6();", "").replace('&lt;br /&gt;', '\n').replace(u'\xa0', u' ').replace(u'\ufffd', u' ') self.write2txt(chapter[0], content_drop) 其中，里面有调用了写入文本的方法：12345# 写入txt，传入章节名和该章节对应的文本内容 def write2txt(self, chapter_name, content): with open('Novels/' + self.NovelTitle + '.txt', 'a') as file: print(self.NovelTitle + ' 正在写入 ' + chapter_name) file.writelines(chapter_name + '\n\n' + content + '\n\n') 所有小说信息在这边，我主要是获取链接页面中，所有小说的基本信息，例如小说名、作者、封面图片链接、内容简介、跳转链接等。首先便是，基本的网页源码读取：12345# 返回页面源码def readHtml(url): html = requests.get(url) # 请求url html.encoding = 'gbk' return html.text 接着是获取该页面上所有小说的基本信息：小说名、作者、封面链接、内容描述、跳转链接，返回信息列表123456789101112131415# 获取小说信息def get_novels_info(url): novels = [] html = readHtml(url) soup = BeautifulSoup(html, 'html.parser') for item in soup.select(".seeWell li"): title = item.find(attrs='clearfix').text # 小说名 author = item.span.find_all('a')[1].text # 作者 coverImg = item.find('img')['src'] # 小说封面链接 desc = item.find('em').text.replace('\n', '').replace(' ', '') # 内容描述 novelUrl = item.span.find('a')['href'] # 小说跳转链接 #print(title, author, coverImg, desc, novelUrl) novel = [title, author, coverImg, desc, novelUrl] novels.append(novel) return novels 这边是小说信息写入csv文件的方法：123456# 将小说信息写入csv文件def write2csv(novels): with open("novels.csv", 'a', newline='') as file: writer = csv.writer(file) for novel in novels: writer.writerow(novel) 之后便是获取小说跳转至目录的链接，并将该链接也添加至数据信息列表，并返回：123456789101112131415161718# 获取小说跳转至章节的链接def get_chapters_url(novel_list): for novel in novel_list: k = 0 obj = None while k &lt; 3: html = readHtml(novel[-1]) soup = BeautifulSoup(html, 'html.parser') obj = soup.find(attrs="reader") if obj: break k += 1 if not obj: novel.append('') continue chapters_url = obj['href'] novel.append(chapters_url) return novel_list 最后便是实例化小说对象，然后获取所有章节链接，之后遍历访问它的小说内容：1234567def all_chapters_url(novel_list): for novel in novel_list: a_novel = ANovel(novel[0], novel[1], novel[2], novel[3], novel[4], novel[5]) # 初始化一个novel对象 a_novel.getChaptersUrl() # 获取该小说对象的所有章节的链接 #print(a_novel.Chapters[:10]) # 打印小说章节列表中的前10章 for chapter in a_novel.Chapters: # 遍历该小说的所有章节 a_novel.getOneChapterContent(chapter) # 获取某一章节的文本内容 调用运行最后就是在main里调用运行即可：1234567if __name__ == '__main__': url = 'http://www.quanshuwang.com/list/1_%s.html' for page in range(1, 2): # range范围可修改 novelsObjList = get_novels_info(url % page) novelsObjList = get_chapters_url(novelsObjList) # write2csv(novelsObjList) all_chapters_url(novelsObjList) 运行显示： 小说下载： 附上程序：https://github.com/asdfv1929/QuanshuNetNovelSpider]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>spider</tag>
        <tag>novel</tag>
        <tag>BeautifulSoup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow实现Softmax Regression识别手写数字]]></title>
    <url>%2F2018%2F01%2F05%2Ftf-softmax-mnist%2F</url>
    <content type="text"><![CDATA[这是本人在学习TensorFlow实战一书时所记录下来的一些内容。 MNIST数据集MNIST（Mixed National Institute of Standards and Technology database）是一个非常简单的机器视觉数据集，如上图所示，它由几万张28像素x28像素的手写数字图片组成，这些图片只包含灰度值信息（channel=1）。我们所需要做的任务就是对这些手写数字的图片进行分类，转成0~9一共10类。 首先对MNIST数据进行加载，TensorFlow为我们提供了一个方便的封装，可以直接加载MNIST数据成我们期望的格式。在Jupyter上运行代码：12345from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets("MNIST_data", one_hot=True)print(mnist.train.images.shape, mnist.train.labels.shape) print(mnist.test.images.shape, mnist.test.labels.shape) print(mnist.validation.images.shape, mnist.validation.labels.shape) 结果显示为：1234567891011Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.Extracting MNIST_data\train-images-idx3-ubyte.gzSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.Extracting MNIST_data\train-labels-idx1-ubyte.gzSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.Extracting MNIST_data\t10k-images-idx3-ubyte.gzSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.Extracting MNIST_data\t10k-labels-idx1-ubyte.gz(55000, 784) (55000, 10)(10000, 784) (10000, 10)(5000, 784) (5000, 10) 数据集成功下载获得，并打印出mnist的训练集中有55000张图像，784维的特征，测试集有10000张图像，验证集有5000张图像。 注：1) 图像是28像素x28像素大小的灰度图片，即空白部分全部为0，有笔迹的地方根据颜色深浅有0到1之间的取值。2) 每个样本有784维的特征，来自于将28x28个像素点展开成一维的结果（28x28=784）。3) 此处简化了问题，丢弃图像的空间结构的信息，将图片按同样的顺序展开至1维向量即可。4) 训练数据的特征是一个55000x784的Tensor，第一个维度是图片的编号，第二个维度是图片中像素点的编号。同时训练的数据Label是一个55000x10的Tensor。测试集、验证集同样道理。5) 读取数据时，对10个种类进行了one-hot编码，Label是一个10维的向量，只有1个值为1，其余为0。比如数字0，对应的Label就是[1,0,0,0,0,0,0,0,0,0]，数字5对应的Label就是[0,0,0,0,0,1,0,0,0,0]，数字n就代表对应位置的值为1。 Softmax Regression数据准备好后，我们采用的是一个叫Softmax Regression的算法来训练手写数字识别的分类模型。数据集的数字都是0~9之间的，所以一共有10个类别。当模型对一张图片进行预测时，Softmax Regression会对每一种类别估算一个概率，比如预测是数字3的概率为80%，是数字5的概率为5%，最后取概率最大的那个数字为模型的输出结果。 $i$代表第$i$类，$j$代表一张图片的第$j$个像素，$b_i$是bias。$$feature_i = \sum_j W_ix_j + b_i$$接下来对所有特征计算Softmax，即计算一个exp函数，然后再进行标准化（让所有类别输出的概率值之和为1）。$$softmax(x) = normalize(exp(x))$$其中判定为第$i$类的概率可由下面公式得到：$$softmax(x)_i = \frac{exp(x_i)}{\sum_jexp(x_j)}$$ 整个Softmax Regression的流程如下图所示：转换为公式的话，如下图所示，将元素相乘变成矩阵乘法：即：上述矩阵表达写成公式的话，就可用下面一行表达：$$y = softmax(Wx + b)$$ TensorFlow实现12345import tensorflow as tfsess = tf.InteractiveSession() # 将InteractiveSession注册为默认的session，之后的运算默认跑到这个session里# 不同session之间的数据和运算都是相互独立的x = tf.placeholder(tf.float32, [None, 784]) # 创建placeholder，输入数据。第一个参数是数据类型，第二个参数是tensor的数据尺寸，这里None代表不限制条数的输入，784维向量 12W = tf.Variable(tf.zeros([784, 10])) # Weights参数，Variable在模型训练迭代中式持久化的。weights初始化为0b = tf.Variable(tf.zeros([10])) # bias全部初始化为0 注：1) 存储数据的tensor一旦使用就会消失，但Variable在模型训练迭代中是持久化的，可长期存在并在每轮迭代中被更新。2) 这里全部初始化为0，因为模型训练时会自动学习合适的值，对这个简单模型来说初始值不太重要。3) 但对于复杂的卷积网络、循环网络或较深的全连接网络，初始化得方法比较重要。123# y = softmax(Wx + b) 改写成TensorFlow语言y = tf.nn.softmax(tf.matmul(x, W) + b) # 实现Softmax算法# tf.matmul 矩阵乘法函数 1234y_ = tf.placeholder(tf.float32, [None, 10]) # 输入真实的label的概率分布 y_# 损失函数cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1])) # reduce_sum求和的cgama， reduce_mean对每个batch数据结果求均值 注：1) 损失函数越小，代表模型的分类结果与真实值的偏差越小，即模型越精确。2) 训练的目的是不断将这个损失减小，直到达到一个全局最优或者局部最优解。3) 对多分类问题，通常使用cross-entropy交叉信息熵来作为损失函数。123# 定义一个优化算法，设置学习率，设定优化目标为cross-entropy# 常用 随机梯度下降SGD算法train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy) # 学习率0.5 12# 使用TensorFlow的全局参数初始化器tf.global_variables_initializer，并直接运行run方法tf.global_variables_initializer().run() 12345# 迭代训练操作train_step# 每次随机从训练集中抽取100条样本构成一个mini-batch，并feed给placeholder，调用train_step进行训练for i in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) train_step.run(&#123;x: batch_xs, y_:batch_ys&#125;) 1correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) 注：argmax获取的是最大值所在位置索引，所以一个是y中概率最大的那类所在位置索引，另一个是真实值y_中最大（即值为1）的位置索引，之后将两者进行equal比较，相等则为true，反之false，最后获得correct_prediction的bool矩阵。1234# 统计全部样本预测的accuracy，先用cast将之前的correct_prediction输出的bool值转换为float32，再求平均# bool转float，即true为 1.0，false为 0.0# 用reduce_mean计算平均值，即为精度值：相等（1.0，即预测正确）的数量在总数量中的占比accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) 12# 将测试数据的特征和label输入评测流程accuracy，计算模型在测试集上的准确率print(accuracy.eval(&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)) 最后的预测精度约为92% 。 附上程序：https://github.com/asdfv1929/TensorFlowInActionLearning (TensorFlow实现Softmax Regression识别手写数字)]]></content>
      <categories>
        <category>TensorFlow实战学习</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>TensorFlow</tag>
        <tag>MNIST</tag>
        <tag>Softmax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Happy New Year]]></title>
    <url>%2F2018%2F01%2F01%2Fnew-year-2018%2F</url>
    <content type="text"><![CDATA[今天是2018年1月1日，新一年的第一天，不写些东西，还真有点不应这个元旦的景。 所谓是新年新气象，希望接下来的一年，能够尽快地完成自己的目标。虽然我知道，这个过程是有点难受，不过不打紧，不停顿、保持往前走的态势就行，希望能够有所收获。 之后希望我的家人，还有大家都平平安安、幸福美满。 asdfv19292018.1.1 14:55]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译：ImageNet Classification with Deep Convolutional Neural Networks]]></title>
    <url>%2F2017%2F12%2F31%2Falexnet%2F</url>
    <content type="text"><![CDATA[此为本人在学习AlexNet时，将论文翻译成中文而作，以翻促学。论文地址：http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf 注：本人也是第一次翻译外文论文，若有纰漏之处，敬请谅解，并可邮箱通知于我[asdfv1929#163.com]进行修改；翻译内容仅供学习使用。 Name DESC TITLE ImageNet Classification with Deep Convolutional Neural Networks AUTHOR Alex Krizhevsky PBYEAR 2012 TRANSLATION asdfv1929 TIME 2017.12 正文翻译： 摘要我们训练了一个大规模的深度卷积神经网络去将ImageNet LSVRC-2010比赛中的120万张高清图像划分到1000个不同的类别中。在测试数据上，我们将top-1和top-5的误差率分别降到了37.5%和17.0%，这比之前的技术水平要好得多。该神经网络，拥有6千万个参数（parameters）和65万个神经元（neurons），包含有5个卷积层（convolutional layers），其中一些卷积层的后面跟着最大池化层（max-pooling layers），还有3个全连接层（full-connected layers）以及一个最后的1000-way softmax函数。为了让训练速度更快，我们采用了非饱和神经元（non-saturating neurons）和一种卷积操作的高效GPU实现方法。为减少全连接层中的过拟合，我们采用了一个最近研究出来的正则化方法（regularization method），叫“dropout”，它被证明是十分有效的。我们也用该神经网络模型的一个变种去参加了ILSVRC-2012比赛，并且同第二名的top-5误差率26.2%相比，我们以top-5误差率15.3%获得了冠军。 引言当前的目标识别方法主要都是利用了机器学习方法。为了提高他们的性能，我们可以收集更大规模的数据集，学习更多强大的模型，采用更优的技术来防止过拟合（overfitting）。直到最近，带有标签（label）的图像数据集的规模仍是相对较小，一般是在万张数量级上（例如NORB，Caltech-101/256，和CIFAR-10/100）。我们可以在这种规模大小的数据集上做简单的识别任务，特别是当它们通过标签保存转换（label-preserving transformations）方法增强了数据。举例来说，当前在MNIST数字识别任务上的最优误差率（&lt;0.3%）已接近人的表现。但是现实环境中的目标对象（object）有着相当大的变化性，所以若要学习去识别它们，就有必要使用更大规模的训练集。事实上，小规模图像数据集的缺点早已被公认，但到最近才可能收集到数百万张带标签的图像数据集。这些新的更大规模的数据集包括有LabelMe，它由数十万张全分割（full-segmented）的图像组成，还有ImageNet，它由超过1500万张带有label的高清图像组成，这些图像有超过22000个种类。 为了从百万张图像中学习数千种目标，我们需要一个具有较大学习能力的模型。然而，目标识别任务的巨大复杂性意味着即使像ImageNet这等规模大的数据集也不能确定该问题，所以我们的模型应采用许多先验知识来弥补我们所没有的数据。卷积神经网络（CNNs）便构成了这样的一类模型。它们的能力可以通过改变它们的深度（depth）和广度（breadth）来控制，并且它们对图像的性质（即，统计上的稳定性和像素依赖的局部性stationarity of statistics and locality of pixel dependencies）也能做出强大且大多正确的假设（预测）。因此，同具有相似规模层（similarly-sized layers）的标准前馈（feedforward）神经网络相比，CNNs拥有更少的连接和参数，因此它们更易于训练，而且它们理论上的最优性能可能仅比前馈神经网络稍差一些。 尽管CNNs具有吸引人的一些特性，而且对于本地架构（local architecture）非常的高效，但它们在大规模应用于高分辨率图像方面上仍然过于昂贵。幸运的是，当前的GPUs，加上二维卷积的高度优化实现方法，足以促进有趣的CNNs的训练，最近的数据集比如说ImageNet，包含了足够的标记样本（labeled examples）来训练此类模型，且不会出现严重的过拟合问题。 本文的主要贡献如下所示：我们在ILSVRC-2010和ILSVRC-2012比赛中使用的ImageNet子集上训练了最大的卷积神经网络之一，并且在这些数据集上获得了迄今为止最好的结果。我们编写了一个二维卷积的高度优化的GPU实现方法，以及其他所有在训练神经网络过程中固有的一些操作，这些我们都公开提供（ http://code.google.com/p/cuda-convnet/ ）。我们的网络包含了一些新的和不寻常的特点，它们可以提高网络的性能，缩短训练时间，具体内容见 Section 3。即使是拥有120万个带标记的训练样本，网络的大小仍然会使得过拟合（overfitting）成为一个严重的问题，所以我们使用了几个有效的技术来防止过拟合，具体信息将在Section 4中介绍。我们最终的网络包含了5个卷积层和3个全连接层，其中深度看上去很重要：我们发现，若移除任意一个卷积层（每个卷积层仅包含不到1%的模型参数）均会导致性能变差。 最后，网络的规模大小主要受限于当前GPUs的可用存储量以及我们能接受的训练时间。我们的神经网络在两台GTX 580 3GB GPUs上训练花费了5至6天的时间。我们所有的实验均表明，只要有更快的GPUs和更大的数据集，我们的实验结果就能进一步提高。 数据集ImageNet是一个拥有超过1500万张带标签的高分辨率图像的数据集，且这些图像大致归属于22000个类别。这些图像收集自网络，并由human labelers使用Amazon的Mechanical Turk crowd-sourceing tool进行人工标记。从2010年开始，作为Pascal Visual Object Challenge的一部分，一个叫ImageNet Large-Scale Visual Recognition（ILSVRC）的比赛开始举办。ILSVRC使用了ImageNet的子集，这个子集中包含了1000个类别，每个类别大约有1000张图像。总之，这个子集大概有120万张训练图像，5万张验证图像，以及15万张测试图像。 ILSVRC-2010是ILSVRC中测试集labels可获得的唯一版本，因此我们是在这个数据集上做了大多数实验。我们也用我们的模型参加了ILSVRC-2012比赛，在Section 6我们会展示关于这个数据集（2012）的实验结果，但其测试集的labels不可获得。在ImageNet中，通常检验这两类误差率：top-1和top-5，其中top-5误差率表示测试图像中的正确label不在模型所认为的可能性最大的5个labels当中的占比。 ImageNet包含了各种分辨率的图像，而我们的系统要求输入数据的维度恒定（constant input dimensionality）。因此，我们对图像进行下采样（down-sample）到一个固定的分辨率 256 x 256。给定一个矩形图像，我们首先缩放图像使得图像的短边长度为256，之后从结果图像中裁剪出中心 256 x 256 大小的块。我们并未使用任何其他方法对图像进行预处理，除了从每个像素中减去训练集的平均活动（subtracting the mean activity over the training set from each pixel）。因此我们是在像素的原始RGB值上训练我们的网络模型。 架构我们网络的架构在图2中总结展示出。它包含了8个学习层–5个卷积层和3个全连接层。接下来，我们将介绍我们网络的架构中几个新的不寻常的特点。Sections 3.1-3.4按照我们对它们重要性的评估进行排序，最重要的排在前列。 ReLU 非线性（Nonlinearity）对神经元输出 f 作为其输入 x 的函数的标准建模方法是 f(x) = tanh(x) 或 f(x) = (1 + e^-x)^-1 。从采用梯度下降（gradient descent）方法的训练时间来看，这些饱和非线性（saturating nonlinearities）是比非饱和非线性（non-saturating nonlinearity）f(x) = max(0, x) 要慢得多。根据Nair和Hinton的想法，我们把具有这种非线性的神经元称为整流线性单元Rectified Linear Units （ReLUs）。使用ReLUs的深度卷积神经网络比使用单元的网络训练速度快上几倍。这在Figure 1中可以看到，上面展示了在一个特定4层卷积网络上对CIFAR-10数据集的训练误差率降到25%所需要的迭代次数。这幅图表明，如果我们使用的是传统的饱和神经元模型，我们将不能够训练出如此大规模的神经网络。 Figure 1：一个采用ReLUs（实线）的4层卷积神经网络达到训练误差率25% 6倍快于带有tanh神经元（虚线）的同等网络。每个网络的学习率都是独立地进行选择以求尽可能快地训练。网络中没有采用任何类型的正则化。这里所展示的效果的量级受网络架构的影响而有可能不同，但是具有ReLUs的网络一直是比带有饱和神经元的网络在学习速度上要快上几倍。 我们并不是第一个考虑在CNNs中替换传统神经元模型的人。就比如说，Jarrett等人声称，非线性 f(x) = |tanh(x)| 在Caltech-101数据集上与局部平均池化（local average pooling）后的对比归一化的合作表现良好（the nonlinearity f(x) = |tanh(x)| works particularly well with their type of contrast normalization followed by local average pooling on the Caltech-101 dataset.）。然而，他们在这数据集上主要关注的是防止过拟合，所以观察到的结果与我们报告中使用ReLUs拟合训练集的加速能力有所不同。更快速的学习能力对在大规模数据集上训练的大型模型的性能具有很大的影响。 在多个GPUs上训练单个GTX 580 GPU只有3GB的内存大小，这限制了能够在其上训练的网络的最大规模。事实证明，120万个训练样本足以训练出网络，但这对于单个GPU来说太大了。因此，我们将网络分布到两个GPUs上。当前的GPUs非常适合跨GPU做并行计算，因为它们能够直接向另一个GPU做读取写入操作，而无需通过主机内存中转。我们所采用的的并行化方案基本上是在每个GPU上放置一半的内核（或神经元），另外还有一个技巧：GPUs之间的通信只在某些层中进行。这意味着，比如说，第3层的内核从第2层的所有内核映射中获取输入。然而，第4层的内核只从第3层中和自己在同一个GPU上的内核映射中获取输入。选择连接的模式对于交叉验证来说是一个问题，但我们可以精确地调整通信量，直到它达到计算量的可接受部分为止。 由此产生的架构有点类似于Cirespan等人使用的“柱状（columnar）”CNN，只是我们的纵列（columns）不是独立的。与在一个GPU上训练的且每个卷积层内核数量减少一半的网络相比，这个方案将我们的top-1和top-5误差率分别降低了1.7%和1.2%。训练双GPU网络的时间较少于单个GPU网络。 局部响应归一化（Local Response Normalization）ReLUs有一个特性使得它们不需要输入归一化来防止它们饱和。如果有一些训练样本产生了正输入（positive input）给ReLUs，就会在那个神经元中进行学习操作。然而，我们仍发现下面这种局部归一化方案有助于一般化（generalization）。假设用 αix,y 表示在 (x, y) 处由第 i 个内核计算而得的神经元的活动，之后应用ReLU非线性，最后响应归一化活动 bix,y 由以下公式定义：其中求和操作是作用于同一空间位置的n个邻近内核映射上，N是层中内核总数。内核映射的顺序是任意的，且在训练开始前就确定好了的。受到真实神经元中的类型所启发，这种响应归一化实现了一种侧向抑制形式（a form of lateral inhibition），为使用不同内核计算得到的神经元输出中的大型活动（big activities）创建竞争机制。常量 k,n,α,β 是超参数，它们的值通过验证集来确定；我们取k=2, n=5, α=10-4, β=0.75 。在特定层应用ReLUs非线性后，我们应用了这种归一化（见Section 3.5）。 该方案与Jarrett等人的局部对比归一化（local contrast normalization）方法有一些相似之处，但是我们的方案更应正确地被命名为“亮度归一化（brightness normalization）”，因为我们没有减去平均活动。响应归一化将我们的top-1和top-5误差率分别下降了1.4%和1.2%。我们也在CIFAR-10数据集上验证了该方案的有效性：未采用归一化的4层CNN取得了13%的测试误差率，采用归一化的只有11%。 重叠池化（Overlapping Pooling）CNNs中的池化层（Pooling layers）汇总了在同一内核映射中相邻神经元组的输出。传统上，由邻近池化单元汇总的邻近关系不会重叠。更准确地说，一个池化层可以被认为是由间隔 s 像素的池化单元组成的网格，每个网格均汇总出以池化单元的位置为中心的大小为 z x z 的邻域关系。如果我们设定 s=z，我们将得到CNNs中常用的传统的局部池化。如果我们设定 s&lt;z，我们将得到重叠池化。这就是我们在网络中所使用的，其中 s=2， z=3。与无重叠的方案 s=2， z=2 相比，这种方案在产生相同维度的输出时分别将top-1和top-5的误差率降低了0.4%和0.3%。我们还观察到，采用重叠池化训练模型会使模型不易出现过拟合问题。 总体架构现在我们开始介绍我们CNN的总体架构。如图2所示，网络中包含了8个加权（weights）的层；前5个是卷积层，余下的3个是全连接层。最后一个全连接层的输出被发送给1000-way softmax上，其产生1000个类别标签的分布。我们的网络使得多项式逻辑回归目标（multinomial logistic regression objective）最大化，这相当于最大化了预测分布下训练样本中正确标签的log概率的平均值。 第2、4、5个卷积层的内核只连接到同一GPU上前一层的那些内核映射上（见Figure 2）。第3个卷积层的内核连接到第2个卷积层的所有内核映射上。全连接层中的神经元与前一层的所有神经元相连。响应归一化层跟在第1、2个卷积层后面。Section 3.4中描述的那种最大池化层跟在两个响应归一化层和第5个卷积层的后面。ReLUs非线性被应用于每个卷积层的输出和全连接层的输出上。 第1个卷积层利用96个大小为 11 x 11 x 3 的内核，采用步长4个像素（同一内核映射中相邻神经元的感受野中心之间的距离），对 224 x 224 x 3 的输入图像做滤波（filter）处理。第2个卷积层将第1个卷积层的（响应归一化、池化后的）输出作为输入，且利用256个大小为 5 x 5 x 48 的内核进行滤波。第3、4、5个卷积层彼此相连，中间没有任何的池化层或归一化层。第3个卷积层有384个大小为 3 x 3 x 256 的内核连接到第2个卷积层的输出上。第4个卷积层有384个大小为 3 x 3 x 192 的内核，第5个卷积层有256个大小为 3 x 3 x 192 的内核。全连接层都各有4096个神经元。 Figure 2：CNN架构的图解，明确地展示出两GPU间的责任划定。一个GPU运行图画中顶部的每层部分，另一个运行图画中底部的每层部分。GPUs间的通信只在某些层中进行。网络的输入是150,528维，网络中剩余层的神经元数量分别是：253,440–186,624–64,896–64,896–43,264–4096–4096–1000。 减少过拟合我们的神经网络架构中拥有6000万个参数。虽然ILSVRC的1000个类别使得每个训练样本在从图像映射到标签label时都强加了10bits的约束（impose 10 bits of constraint on the mapping from image to label），但是这不足以在学习如此多的参数情况下而没有大量的过拟合（在学习这么多的参数情况下必定会有过拟合问题）。下面，我们介绍两个对抗过拟合的主要方法。 数据增强（Data Augmentation）在图像数据上减少过拟合的最简单和最常见的方法是使用标签保存转换（label-preserving transformations）方法人为地扩大数据集规模。我们采用了两个不同的数据增强方式，两者都允许以少量的计算从原始图像中生成转换图像，所以转换图像不需要存储在硬盘上。在我们的实现中，转换图像是由CPU上的Python代码生成的，而GPU正在训练前一batch的图像。因此这些数据增强方案事实上是计算自由的（computationally free）。 第一种数据增强方案包括了生成图像翻译（image translations）和水平反射（horizontal reflections）。为此，我们从 256 x 256 的图像中提取随机的 224 x 224 区块（patches）（和它们的水平反射图像），并在这些提取出来的区块上训练我们的网络。这使得我们的训练集增加了2048倍（（256-224）^2 * 2），尽管由此产生的训练样本相互之间高度关联。若没有这种方案，我们的网络将遭受严重的过拟合问题，这就会迫使我们采用更小规模的网络。在测试过程中，网络通过提取5个 224 x 224 区块（4个角落区块和一个中央区块）和它们的水平反射（因此共有10个区块）来做预测，并且对由网络softmax层对这10个区块的预测值做平均处理。 第二种数据增强方案包括了更改训练图像的RGB通道的强度（altering the intensities of the RGB channels）。具体来说，我们在整个ImageNet训练集上的RGB像素值集合上都采用了PCA方法。对于每个训练图像，我们成倍增加已有的主要成分，比例大小为对应特征值乘上一个从均值0标准差0.1的高斯分布中提取的随机变量。因此对于每个RGB图像像素，我们添加如下：其中是RGB像素值的 3 x 3 协方差矩阵的第 i 个特征向量和特征值，αi是前面提到的随机变量。每个αi对于一个特定的训练图像的所有像素仅被绘制一次，直到该图像被再次用来训练，此时它才会被再次绘制。这种方案近似捕捉到了自然图像的一个重要特性，即，目标身份不会随光照的强度和颜色的变化而改变。该方案将top-1误差率降低了1%。 Dropout结合众多不同模型的预测是一个减少测试误差的不错的方式，但是这对于大型神经网络来说仍过于昂贵，得花上几天时间来训练。然而，有这么一个高效的模型组合版本，只花费两倍的在单个模型上的训练时间。该最近引入的技术，称为“dropout”，以0.5的概率将每个隐藏神经元的输出置为0。以这种方式被“drop out”的神经元既不对前向传播（forward pass）做贡献也不参与反向传播（backpropagation）。所以每次提交输入时，神经网络都采用不同的架构，但所有的架构共享权值。这种技术降低了神经元复杂的互适应关系，因为一个神经元不能依赖于其他特定神经元的存在。因此，它被迫学习更多健壮的特征，这些特征在与其他神经元的不同随机子集相连时是非常有用的。在测试时，我们使用所有的神经元，但它们的输出乘以0.5，对于获取指数级dropout网络产生的预测分布的几何平均值，这是一种合理的近似（a reasonable approximation to taking the geometric mean of the predictive distributions produced by the exponentially-many dropout networks）。 我们在Figure 2的前两个全连接层上采用了dropout。若没有dropout，我们的网络会表现出实质性的过拟合。Dropout使收敛所需的迭代次数大致增加了一倍。 学习的细节我们采用了随机梯度下降（stochastic gradient descent）方法，并令batch size = 128，momentum = 0.9，weight decay = 0.0005，对模型进行训练。我们发现这个小量的weight decay对于模型学习是十分重要的。换句话说，这儿的weight decay不仅仅是一个regularizer：它减少了模型的训练误差。权值weight w的更新规则是：其中 i 是迭代索引，v 是momentum变量，ε是学习率，是第 i 个batch Di上的目标L在 wi 处对 w 的偏导数的平均值。 我们用一个均值为0、标准差为0.01的高斯分布初始化了每一层的权值。我们将第2、4、5个卷积层和全连接隐层的神经元偏移量biases初始化为常量1。这种初始化通过向ReLUs提供正输入来加速学习的早期阶段。 我们将其余层中的神经元偏差biases初始化为0。 我们对所有图层使用了相同的学习率，它是由我们在训练过程中手动调节出来的。我们所遵循的启发式是当验证误差率在当前学习率下不再提升（数字下降）时，就将学习率除以10。学习率初始化为0.01，且在训练终止前下降3次。我们用120万张图像的训练集对网络进行了大约90个周期的训练，在两台NVIDIA GTX 580 3GB GPU上花费了五到六天的时间。 结果关于ILSVRC-2010数据集的结果已总结展示在表1中。我们训练的网络在top-1和top-5测试集误差率上分别是37.5%和17.0%。在ILSVRC-2010比赛中取得的最好结果是47.1%和28.2%，它是通过由不同特征训练出来的6个稀疏编码（sparse-coding）模型产生的预测值求平均而得；之后，所公布的最好结果是45.7%和25.7%，它是在由两种类型密集采样特征（densely-sampled features）计算得到的Fisher向量（Fisher Vectors）上训练出来的分类器的预测值求平均。 我们同样将我们的模型参加了ILSVRC-2012比赛，其中获得的结果如表2所示。由于ILSVRC-2012测试集的labels并未公开提供，因此我们不能报告展示出我们尝试的所有模型的测试误差率。在本段的剩余部分，我们使用验证误差率来替换测试误差率，因为根据我们的经验，它们之间的差异不会超过0.1%（见表2）。本文介绍的CNN其top-5误差率达到了18.2%。5个相似CNN的top-5误差率平均值是16.4%。训练一个在最后池化层后加上第6个卷积层的CNN，去对整个ImageNet Fall 2011版本数据集（1500万张图像，22000个类别）进行分类，然后对其进行调优，在ILSVRC-2012上的top-5误差率达到16.6%。两个对Fall 2011数据集预训练的CNN，加上前面所提到的5个CNN，对它们的预测值求平均后，top-5误差率达到15.3%。比赛中第二名的top-5达到26.2%，他的方法是在几个不同类型的密度采样特征（densely-sampled features）计算得到的FVs上训练出的分类器的预测值的平均值。 最后，我们也展示了在ImageNet Fall 2009版本数据集（10184个类别，890万张图像）上的误差率。在这个数据集上，我们遵循文献中使用一半图像做训练和一半做测试的惯例。由于没有建立测试集，我们的数据集划分应该与之前的作者使用的划分有所不同，但是这并不影响结果。在这数据集上的top-1和top-5误差率达到67.4%和40.9%，该结果是由上述网络（在最后池化层后带有一个额外的第6个卷积层）所得到。在该数据集上已公布的最优结果是78.1%和60.9%。 定性评估（Qualitative Evaluations）Figure 3展示了通过网络的两个数据连接层学习得到的卷积内核。该网络已学习了各种频率选择（frequency-selective）和方向选择（orientation-selective）的内核，以及各种色块斑点（colored-blobs）。考虑到由两个GPUs展示的专业化，受限连通性（restricted connectivity）的结果呈现在Section 3.5中。GPU 1上的内核很大程度上与颜色无关（ color-agnostic），而GPU 2上的内核与颜色密切相关（ color-specific）。这种专业化发生在每一次运行当中，并且是独立于任何特定的随机权重初始化（GPUs重新编号）。 Figure 3：由第1个卷积层在 224 x 224 x 3 输入图像上训练得到的96个大小为 11 x 11 x 3 的卷积内核。上边的48个内核是在GPU 1上学习得到，下边的48个内核是在GPU 2上学习得到。具体内容见Section 6.1。 Figure 4：（左）8张ILSVRC-2010测试图像和由模型认为的最有可能的5个标签。正确的标签显示在每张图像的下方，分配给正确标签的概率也用红色框标注显示在上图中（如果正确标签在预测的5个标签当中）。（右）第一列上有5张ILSVRC-2010测试图像。其余列上展示的是与测试图像在最后一个隐层上的特征向量具有最短欧氏距离的6张训练图像。 在Figure 4的左半部分，我们通过计算网络模型在8张测试图像上的top-5预测值来定性地评估（网络）学到了什么。请注意，即使是偏离中心位置的目标，比如左上角的那只螨虫，仍被网络所识别出来。大部分top-5的标签（labels）看起来都很合理。比如说，只有一些其他类别的猫科动物被错误认为是豹类。在某些情况下（汽车护栅，樱桃），网络对照片中究竟应关注哪个目标对象存在着歧义。 探索网络视觉内容的另一种方法是考虑由最后一个图像（4096维度的隐藏层）引起的特征激活（feature activations）。如果两张图像的特征激活向量具有较小的欧氏距离，则可以说神经网络认为它们在很大程度上相似。Figure 4右半部分中展示了测试集中的5张图像，还有根据上述方法找出来的与这5张图像中每个图像最相似的6张训练集中的图像。请注意，在像素级别上，所检索到的训练图像一般是不与第一列中的查询图像在L2上相近。比如说，所检索的dogs和elephants图像中它们有各种各样的姿势。我们在补充材料里提供了更多的测试图像的结果。 在两个4096维的实值向量之间使用欧氏距离来计算相似度的效率十分低，但是通过训练自动编码器（auto-encoder）来将这些向量压缩成短二进制码（short binary codes）将会使之变得高效。这应该会产生出一种比对原始像素应用自编码更好的图像检索方法，因为不需要用到图像标签labels，因此倾向于检索出带有相似边缘模式的图像，无论它们在语义上是否相似。 讨论我们的结果展示出：一个大型深度卷积神经网络能够在一个极具挑战的数据集上使用纯监督学习（supervised learning）而取得破纪录的成绩结果。值得注意的是，如果移除一个卷积层，我们网络的性能就会下降。比如，移除任意一个中间层就会导致网络的top-1误差率增加2%。所以深度（depth）对于我们获得结果非常重要。 为了简化实验，我们没有使用任何无监督的预训练，即使我们知道这样会有帮助，特别是当我们获得了足够的计算能力去大幅度提升网络的规模而不去对应地增加带有label的数据时。至此，我们的结果已经得到优化，因为我们已让网络规模更大，训练它们的时间更长，但是我们仍有许多数量级（many orders of magnitude）要去做，以求匹配上人类视觉系统的神经-时间通路（infero-temporal pathway）。最后，我们希望在视频序列上运用大型深度卷积神经网络，因为视频序列的时序结构提供了在静态图像中丢失或者不明显的有用信息。]]></content>
      <categories>
        <category>Papers</category>
      </categories>
      <tags>
        <tag>Paper</tag>
        <tag>Translation</tag>
        <tag>Deep Learning</tag>
        <tag>CNN</tag>
        <tag>AlexNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用GitHub+Node.js+Hexo搭建个人博客]]></title>
    <url>%2F2017%2F11%2F18%2Fhexo-next-blog%2F</url>
    <content type="text"><![CDATA[本篇是自己在搭建Hexo博客平台时的一个过程记录。 GitHub账号注册因为此文所搭建的个人博客是基于GitHub平台服务的，所以首先是注册GitHub，当然已有账号的跳过此步； 传送门：GitHub官网，页面如下，输入username、email、password后点击Sign Up注册，当然这些之后是需要去对应的邮箱邮件里进行注册激活。页面跳转至如下，这边我们是点击Continue，默认里面其他设置。页面如下，根据问题勾选自己对应的。 创建Repository基本配置完成后，跳转至如下，这边我们就直接Start a project了，你也可以选择先看下guide，点击开始后，跳转至Create a new repository，这边Repository name命名规则就是username.github.io，其中username就是你注册时的username。 Settings点击创建之后页面跳转至project内，这时我们点击settings这边就是setting下这时我们在该页面往下拖动网页，找到GitHub Pages，之后点击Choose a theme选择页面主题，这边我们就暂时选择默认的主题（因为后面我会更换为NexT主题的），然后点击Select theme。 GitHub Page点击选择主题后，页面会跳转至该Repository的可以说是主页吧，如下所示。上面有提醒主题更换，也有生成一个index.mdmarkdown文件。这时我们再去刚才的setting设置里去看刚刚的GitHub Pages那边，会有显示你的url，这就是你未来博客“搭建”在该网址上。此时点击该url访问到的也就是你未来个人博客将会展示的样子 Node.Js下载安装传送门：NodeJs，选择对应的版本进行下载，安装的话就点点点，这边就不在叙述了。软件安装完成后，打开cmd界面，输入node -v和npm -v（注意查看环境变量部分是否已经正确），查验其版本，确认是否安装成功。 安装Hexo在命令行输入：1npm install -g hexo-cli 之后创建一个文件夹（搭建博客的相关文件存放于此），此处我命名为test（可自主命名），在该文件夹路径下打开cmd（在文件夹下的路径输入框中输入cmd并回车），或者直接在cmd中切换到该文件夹下，输入：1hexo init 运行后的样子，其中在下载默认主题landscape时，出现了一些乱码，不过这些问题不大，最后显示添加add成功就行：此时文件夹下多了如下一些内容： 本地运行此时还是在该文件夹路径下的cmd里输入（-p 5000 表示设置端口号为5000；若不写这个，则默认为4000，但可能会出现端口号被占用的情况，导致网页无法打开）：1hexo server -p 5000 此时浏览器上输入http://localhost:5000/，访问到博客的默认界面： hexo相关命令新建某一博客，文件名为this_is_a_test_blog.md，此为markdown文件，文件路径为source\_posts\1hexo new this_is_a_test_blog 我们可以在这this_is_a_test_blog.md文件里添加自己的博客内容，因为我是用作测试的，所以什么内容这边就不贴图了。重新运行hexo server -p 5000，访问localhost可以看到，网页中有了内容添加。 其他相关的命令如下：hexo clean 删除public文件夹及其内容（public文件夹的内容即为上传至GitHub Repositoriy的内容）hexo generate或hexo g 生成上传至GitHub的内容，即public文件夹hexo deploy或hexo d 上传至GitHub（需进行配置） 这时候，博客的基本设置（主题设置留待以后）都已弄好，接下来就是解决将博客内容上传至GitHub上的问题。 Git下载安装传送门：git，选择对应版本下载安装。安装过程这边也不叙述了，就点点点。安装完成后，看下版本git --version 配置个人信息打开Git Bash，配置个人信息，分别输入如下命令，yourname即GitHub注册时的用户名，yourEmail即注册时的邮箱账号，以及生成秘钥：1234git config --global user.name &quot;yourname&quot;git config --global user.email &quot;yourEmail&quot;ssh-keygen -t rsa -C &quot;yourEmail&quot; 在秘钥生成后，会有对应的存放文件地址，去该地址中，找到id_rsa.pub文件，复制里面的内容，粘贴至GitHub中，点击右上角用户头像下的Settings，之后点击左侧的SSH and GPG keys，找到New SSH key点击，输入title，并将之前复制的内容粘贴到下面的key中，最后点击Add SSH key，完成： 配置Deployment去博客总目录下的_config.yml文件中，找到deploy部分，添加如下（yourname就是GitHub的用户名）：1234deploy: type: git repo: https://github.com/yourname/yourname.github.io.git branch: master 整体流程至此，除了博客主题（目前采用默认主题）外，其他基本设置都已弄好。整体的写文上传等操作流程为：hexo new newBloghexo cleanhexo ghexo d]]></content>
      <categories>
        <category>GitBlog</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>Hexo</tag>
        <tag>GitHub</tag>
        <tag>Node.js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中秋节快乐！]]></title>
    <url>%2F2017%2F10%2F04%2Fmidautumn%2F</url>
    <content type="text"><![CDATA[祝所有走过路过的亲们 中秋节快乐，幸福如意！时间：2017-10-04节日：中秋佳节地点：笔者家]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[选择排序和冒泡排序]]></title>
    <url>%2F2017%2F09%2F21%2Fbruteforce%2F</url>
    <content type="text"><![CDATA[本小节主要关于选择排序和冒泡排序的分析 选择排序基本原理 扫描整个列表或数组（长度为n），找到最小元素，将其和第一个元素进行交换，此时最小元素在有序表中的最终位置上； 从第二元素开始扫描列表，找到最后n-1个元素中的最小元素，再和第二个元素交换位置； 之后，在对该列表做第i遍扫描时(i值为0~n-2)，在最后n-i个元素中寻找最小元素，然后将之和 A[i] 交换； 在经过n-1遍后，列表已序。 示例：1234567| 89 45 68 90 29 34 17 17 | 45 68 90 29 34 89 17 29 | 68 90 45 34 89 17 29 34 | 90 45 68 89 17 29 34 45 | 90 68 89 17 29 34 45 68 | 90 89 17 29 34 45 68 89 | 90 扫描这个7个数，找到最小数17，然后将其与第一个元素89进行交换，此时17已序；之后扫描剩下的6个数，找到其中最小数29，将其与45交换；接着依次找到34，45, 68，89，最后数组已序。 代码实现伪代码： 12345678910算法 SelectionSort(A[0..n-1]) //该算法用选择排序对给定列表排序 //输入: 一个可排序数组 A[0..n-1] //输出: 升序排列的数组 A[0..n-1] for i = 0 to n - 2 do min = i for j = i + 1 to n - 1 do if A[j] &lt; A[min] min = j swap A[i] and A[min] python实现： 1234567891011def SelectionSort(A): #选择排序 对给定列表排序(升序) #输入: 待排序列表 A[0..n-1] list #输出: 已排序列表 A[0..n-1] list lenA = len(A) for i in range(lenA-1): minValIndex = i for j in range(i+1, lenA): if A[minValIndex] &gt; A[j]: minValIndex = j A[i], A[minValIndex] = A[minValIndex], A[i] java实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package selection_bubble_sort;/** * @author jwj * */public class SelectionSort &#123; /** * @param arr */ public static void selectSort(int[] arr) &#123; int min = -1; for (int i = 0; i &lt; arr.length-1; i++) &#123; min = i; for (int j = i+1; j &lt; arr.length; j++) &#123; if (arr[j] &lt; arr[min]) &#123; min = j; &#125; &#125; swap(arr, i, min); &#125; &#125; /** * @param array * @param i * @param j */ public static void swap(int[] array, int i, int j) &#123; int temp = array[i]; array[i] = array[j]; array[j] = temp; &#125; /** * @param array */ public static void print(int[] array) &#123; for (int i : array) &#123; System.out.print(i + " "); &#125; &#125; /** * @param args */ public static void main(String[] args) &#123; int[] a = &#123;4, 2, 3, 6, 1, 7, 9, 8&#125;; selectSort(a); print(a); &#125;&#125; 算法分析选择排序的输入规模是由数组中元素的个数n决定的。整个算法的基本操作是键值比较$A[j] &lt; A[min]$。比较的执行次数：$$C(n) = \sum_{i=0}^{n-2} \sum_{j=i+1}^{n-1} 1 = \sum_{i=0}^{n-2} [(n-1)-(i+1)+1] = \sum_{i=0}^{n-2}(n-1-i) = \frac{(n-1)n}{2}$$因此，对于任何输入来说，选择排序都是一个$\Theta(n^2)$的算法，其中数组元素交换的次数仅为$\Theta(n)$，更精确点，是$n-1$次（$i$循环每重复依次执行一次交换），这个特性使得选择排序优于许多其他的排序算法。 冒泡排序基本原理 从第一个元素开始，依次比较相邻元素值，即第一个元素和第二个元素比较，若逆序，则交换位置； 第二个元素和第三个元素比较，若逆序，交换位置； 依次下去，则将最大元素移动到了最右端（“沉到最底部”）； 再次从列表头开始依次比较相邻数，重复多次后，第二大的数已移到最右第二个位置上； 多次比较后，列表已序。 示例：1234567891011121314151689 &lt;?&gt; 45 68 90 29 34 1745 89 &lt;?&gt; 68 90 29 34 1745 68 89 &lt;?&gt; 90 29 34 1745 68 89 90 &lt;?&gt; 29 34 1745 68 89 29 90 &lt;?&gt; 34 1745 68 89 29 34 90 &lt;?&gt; 1745 68 89 29 34 17 |9045 &lt;?&gt; 68 89 29 34 17 |9045 68 &lt;?&gt; 89 29 34 17 |9045 68 89 &lt;?&gt; 29 34 17 |9045 68 29 89 &lt;?&gt; 34 17 |9045 68 29 34 89 &lt;?&gt; 17 |9045 68 29 34 17 |89 90etc. 首先，比较数组中的第一、第二个元素值，89和45，逆序，交换位置；之后比较第二、第三个元素值，89和68，逆序，交换位置；接着比较第三、第四个元素值，89和90，已序，不变；然后继续依次比较后续的数值，直到最大数90移动到最右端为止。再次从数组开头依次进行比较，最后将第二大的数值移动到最右第二个位置上。之后就继续多次比较，直到所有数据已序。 代码实现伪代码：12345678算法 BubbleSort(A[0..n-1]) //该算法用冒泡排序对数组A[0..n-1]进行排序 //输入：一个可排序数组A[0..n-1] //输出：非降序排列的数组A[0..n-1] for i = 0 to n - 2 do for j = 0 to n - 2 - i do if A[j+1] &lt; A[j] swap A[j] and A[j+1] python实现：123456789def BubbleSort(A): #冒泡排序 对给定列表排序(升序) #输入: 待排序列表 A[0..n-1] list #输出: 已排序列表 A[0..n-1] list lenA = len(A) for i in range(lenA-1): for j in range(lenA-1-i): if A[j+1] &lt; A[j]: A[j], A[j+1] = A[j+1], A[j] java实现：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package selection_bubble_sort;/** * @author jwj * */public class BubbleSort &#123; /** * @param arr */ public static void bubbleSort(int[] arr) &#123; int len = arr.length; for (int i = 0; i &lt; len-1; i++) &#123; for (int j = 0; j &lt; len-1-i; j++) &#123; if (arr[j+1] &lt; arr[j]) &#123; swap(arr, j, j+1); &#125; &#125; &#125; &#125; /** * @param array * @param i * @param j */ public static void swap(int[] array, int i, int j) &#123; int temp = array[i]; array[i] = array[j]; array[j] = temp; &#125; /** * @param arr */ public static void print(int[] arr) &#123; for (int i : arr) &#123; System.out.print(i + " "); &#125; &#125; /** * @param args */ public static void main(String[] args) &#123; int[] array = &#123;5, 7, 4, 3, 6, 1, 2, 9, 8&#125;; bubbleSort(array); print(array); &#125;&#125; 算法分析对于所有规模为n的数组来说，该冒泡排序版本的键值比较次数都是相同的，次数：$$C(n) = \sum_{i=0}^{n-2} \sum_{j=0}^{n-2-i} 1 = \sum_{i=0}^{n-2} [(n-2-i)-0+1] = \sum_{i=0}^{n-2}(n-1-i) = \frac{(n-1)n}{2}$$键交换次数取决于特定的输入。最坏的情况是遇到降序排列的数组，这时，键交换次数和键比较次数是相同的。$$S_{worst}(n) = C(n) = \frac{(n-1)n}{2}$$]]></content>
      <categories>
        <category>算法设计与分析</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Selection Sort</tag>
        <tag>Bubble Sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一篇博文]]></title>
    <url>%2F2017%2F09%2F16%2Ffirstblog%2F</url>
    <content type="text"><![CDATA[因为感觉自己若能有一个独立的博客网站，能够在上面写写东西，能够把自己学到的东西给记录下来，这绝对是一件很cool的事，所以就有了这个博客网站。因此自己特地花上两三天时间，终于把一个能够搬上台面的博客网站给整出来了。当然，目前网站里也只是完成了一些基本功能的整合，还有许多常用的功能尚未组装进来，所以后续的会继续进行完善。 写这个博文，第一是想庆祝下，自己这几天的忙绿终有结果，看着还不错 (￣▽￣)~*第二就是想提醒下自己，在以后的学习成长过程中最好能够把所看到的、所学到的内容整理出来，方便理清思路及以后的回顾，搭建这个博客的初衷也就是这；最后，希望自己能够稳步前行。 over！]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
</search>
